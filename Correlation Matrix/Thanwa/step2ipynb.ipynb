{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"step1_student_data.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1NG1zPbJTgl8GwuIAMicEZm9Fb07nCx5H\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Pandas ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏¢‡∏≤‡∏ß ‡πÜ **\n",
        "# *******************************************************************\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "# *******************************************************************\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• student-mat.csv **\n",
        "# *******************************************************************\n",
        "\n",
        "# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ã‡∏°‡∏¥‡πÇ‡∏Ñ‡∏•‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô\n",
        "df = pd.read_csv('student-mat.csv', sep=';')\n",
        "\n",
        "# 2. ‡∏ó‡∏≥ One-Hot Encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Categorical)\n",
        "# ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏ä‡πà‡∏ô traveltime, studytime) ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Target Column (‡πÉ‡∏ä‡πâ G3 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)\n",
        "TARGET_COLUMN = 'G3'\n",
        "\n",
        "# X_df ‡∏Ñ‡∏∑‡∏≠ Features (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢)\n",
        "X_df = df_encoded.drop(TARGET_COLUMN, axis=1)\n",
        "\n",
        "# y_series ‡∏Ñ‡∏∑‡∏≠ Target (‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ G3 ‡πÄ‡∏õ‡πá‡∏ô Binary: Pass/Fail\n",
        "# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠ > 10\n",
        "y = (df_encoded[TARGET_COLUMN] > 10).astype(int).values\n",
        "\n",
        "# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô\n",
        "X = X_df.values\n",
        "feature_names = X_df.columns.tolist()\n",
        "\n",
        "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\")\n",
        "print(X_df.head())\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "\n",
        "print(\" MinMax (Manual Scaling) \")\n",
        "# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Min-Max Scaling ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á\n",
        "denominator = X_max - X_min\n",
        "# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà X_max == X_min (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå)\n",
        "X_scaled = np.where(denominator == 0, 0, (X - X_min) / denominator)\n",
        "# --- ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Scale ‡πÅ‡∏•‡πâ‡∏ß\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà Scale ‡πÅ‡∏•‡πâ‡∏ß (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\")\n",
        "print(df_scaled.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Variance \")\n",
        "variances = X_scaled.var(axis=0)\n",
        "# ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏à‡∏∂‡∏á‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å\n",
        "print(\"Top 5 Variances:\")\n",
        "for i in np.argsort(variances)[-5:]:\n",
        "    print(feature_names[i], \":\", variances[i])\n",
        "print(\"\\n\")\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô **\n",
        "# *******************************************************************\n",
        "print(\"--- Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ---\")\n",
        "# ‡πÅ‡∏™‡∏î‡∏á Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "correlation_matrix = df_scaled.corr()\n",
        "print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix:\", correlation_matrix.shape)\n",
        "print(correlation_matrix)\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ: ‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Correlation-Based Feature Selection) **\n",
        "# *******************************************************************\n",
        "\n",
        "def select_features_by_correlation(df, threshold=0.85):\n",
        "    \"\"\"\n",
        "    ‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
        "    ‡πÇ‡∏î‡∏¢‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Standard Deviation (S.D.) ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏ß‡πâ\n",
        "    \"\"\"\n",
        "    print(\"--- üî¥ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Threshold: > %.2f) üî¥ ---\" % threshold)\n",
        "    df_current = df.copy()\n",
        "\n",
        "    round_count = 1\n",
        "\n",
        "    while True:\n",
        "        # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "        corr_matrix = df_current.corr().abs()\n",
        "\n",
        "        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏ô‡∏ß‡∏ó‡πÅ‡∏¢‡∏á‡∏°‡∏∏‡∏°)\n",
        "        np.fill_diagonal(corr_matrix.values, 0)\n",
        "\n",
        "        max_corr_value = corr_matrix.max().max()\n",
        "\n",
        "        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (Threshold)\n",
        "        if max_corr_value <= threshold:\n",
        "            print(\"\\n‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå (\", threshold, \")\")\n",
        "\n",
        "            # ** ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Correlation Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **\n",
        "            final_corr_matrix = df_current.corr()\n",
        "            print(\"\\n--- Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---\")\n",
        "            print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢:\", final_corr_matrix.shape)\n",
        "            print(final_corr_matrix)\n",
        "\n",
        "            break\n",
        "\n",
        "        # ‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "        r, c = np.unravel_index(corr_matrix.values.argmax(), corr_matrix.shape)\n",
        "        feature_a = corr_matrix.columns[r]\n",
        "        feature_b = corr_matrix.columns[c]\n",
        "\n",
        "        print(\"\\n--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà %d ---\" % round_count)\n",
        "\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1)\n",
        "        print(\"1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î %.4f):\" % max_corr_value)\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "        print(corr_matrix.loc[[feature_a, feature_b], [feature_a, feature_b]])\n",
        "\n",
        "        # 4. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (Standard Deviation)\n",
        "        std_a = df_current[feature_a].std()\n",
        "        std_b = df_current[feature_b].std()\n",
        "\n",
        "        print(\"\\n3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\")\n",
        "        print(\"   * **%s**: S.D. = **%.5f**\" % (feature_a, std_a))\n",
        "        print(\"   * **%s**: S.D. = **%.5f**\" % (feature_b, std_b))\n",
        "\n",
        "        # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à: ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ S.D. ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\n",
        "        if std_a >= std_b:\n",
        "            feature_to_drop = feature_b\n",
        "            feature_to_keep = feature_a\n",
        "        else:\n",
        "            feature_to_drop = feature_a\n",
        "            feature_to_keep = feature_b\n",
        "\n",
        "        # ‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ S.D. ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á\n",
        "        df_current = df_current.drop(columns=[feature_to_drop])\n",
        "\n",
        "        # 4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à\n",
        "        print(\"4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\")\n",
        "        print(\"   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: '%s' (S.D. **%.5f**)\" % (feature_to_keep, max(std_a, std_b)))\n",
        "        print(\"   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: '%s' (S.D. %.5f)\" % (feature_to_drop, min(std_a, std_b)))\n",
        "\n",
        "        round_count += 1\n",
        "\n",
        "    print(\"--- üèÅ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ üèÅ ---\")\n",
        "    print(\"‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠:\", list(df_current.columns))\n",
        "    return df_current\n",
        "\n",
        "# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞\n",
        "df_selected = select_features_by_correlation(df_scaled, threshold=0.90)\n",
        "X_selected = df_selected.values\n",
        "feature_names_selected = df_selected.columns.tolist()\n",
        "\n",
        "print(\"\\n--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ ---\")\n",
        "print(\"‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:\", feature_names_selected)\n",
        "\n",
        "# ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
        "X_train, X_test, y_train, y_test_sel = train_test_split(X_selected, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_sel = model.predict(X_test)\n",
        "acc_sel = accuracy_score(y_test_sel, y_pred_sel)\n",
        "print(\"Accuracy (Features Selected by Correlation/SD):\", acc_sel)\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "\n",
        "print(\" Features \")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_1 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å Features):\", acc_1)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Threshold = 0.03\")\n",
        "selector = VarianceThreshold(threshold=0.03)\n",
        "X_new_2 = selector.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new_2, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_2 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc_2)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Best Features (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° Variance)\")\n",
        "# ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Variance ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "sorted_idx = np.argsort(variances)\n",
        "top3_idx = sorted_idx[-3:]\n",
        "X_best3 = X_scaled[:, top3_idx]\n",
        "X_train, X_test, y_train, y_test_3, = train_test_split(X_best3, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_3 = model.predict(X_test)\n",
        "acc_3 = accuracy_score(y_test_3, y_pred_3)\n",
        "print(\"Accuracy (Top 3):\", acc_3)\n",
        "print(\"\\n\")\n",
        "\n",
        "# ... (‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTIRLv1d6_oo",
        "outputId": "dbbc01db-7c5f-413e-b978-1e388aaa1870"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\n",
            "   age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  goout  Dalc  Walc  health  absences  G1  G2  school_MS  sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  Fjob_teacher  reason_home  reason_other  reason_reputation  guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  activities_yes  nursery_yes  higher_yes  internet_yes  romantic_yes\n",
            "0   18     4     4           2          2         0       4         3      4     1     1       3         6   5   6      False  False       True        False      False        False       False          False         False        False       False          False          True        False         False              False             True           False           True       False     False           False         True        True         False         False\n",
            "1   17     1     1           1          2         0       5         3      3     1     1       3         4   5   5      False  False       True        False       True        False       False          False         False        False        True          False         False        False         False              False            False           False          False        True     False           False        False        True          True         False\n",
            "2   15     1     1           1          2         3       4         3      2     2     3       3        10   7   8      False  False       True         True       True        False       False          False         False        False        True          False         False        False          True              False             True           False           True       False      True           False         True        True          True         False\n",
            "3   15     4     2           1          3         0       3         2      2     1     1       5         2  15  14      False  False       True        False       True         True       False          False         False        False       False           True         False         True         False              False             True           False          False        True      True            True         True        True          True          True\n",
            "4   16     3     3           1          2         0       4         3      2     1     2       5         4   6  10      False  False       True        False       True        False        True          False         False        False        True          False         False         True         False              False            False           False          False        True      True           False         True        True         False         False\n",
            "\n",
            "\n",
            " MinMax (Manual Scaling) \n",
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà Scale ‡πÅ‡∏•‡πâ‡∏ß (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\n",
            "        age  Medu  Fedu traveltime studytime failures famrel freetime goout  Dalc  Walc health  absences      G1        G2 school_MS sex_M address_U famsize_LE3 Pstatus_T Mjob_health Mjob_other Mjob_services Mjob_teacher Fjob_health Fjob_other Fjob_services Fjob_teacher reason_home reason_other reason_reputation guardian_mother guardian_other schoolsup_yes famsup_yes paid_yes activities_yes nursery_yes higher_yes internet_yes romantic_yes\n",
            "0  0.428571   1.0   1.0   0.333333  0.333333      0.0   0.75      0.5  0.75   0.0   0.0    0.5      0.08   0.125  0.315789       0.0   0.0       1.0         0.0       0.0         0.0        0.0           0.0          0.0         0.0        0.0           0.0          1.0         0.0          0.0               0.0             1.0            0.0           1.0        0.0      0.0            0.0         1.0        1.0          0.0          0.0\n",
            "1  0.285714  0.25  0.25        0.0  0.333333      0.0    1.0      0.5   0.5   0.0   0.0    0.5  0.053333   0.125  0.263158       0.0   0.0       1.0         0.0       1.0         0.0        0.0           0.0          0.0         0.0        1.0           0.0          0.0         0.0          0.0               0.0             0.0            0.0           0.0        1.0      0.0            0.0         0.0        1.0          1.0          0.0\n",
            "2       0.0  0.25  0.25        0.0  0.333333      1.0   0.75      0.5  0.25  0.25   0.5    0.5  0.133333    0.25  0.421053       0.0   0.0       1.0         1.0       1.0         0.0        0.0           0.0          0.0         0.0        1.0           0.0          0.0         0.0          1.0               0.0             1.0            0.0           1.0        0.0      1.0            0.0         1.0        1.0          1.0          0.0\n",
            "3       0.0   1.0   0.5        0.0  0.666667      0.0    0.5     0.25  0.25   0.0   0.0    1.0  0.026667    0.75  0.736842       0.0   0.0       1.0         0.0       1.0         1.0        0.0           0.0          0.0         0.0        0.0           1.0          0.0         1.0          0.0               0.0             1.0            0.0           0.0        1.0      1.0            1.0         1.0        1.0          1.0          1.0\n",
            "4  0.142857  0.75  0.75        0.0  0.333333      0.0   0.75      0.5  0.25   0.0  0.25    1.0  0.053333  0.1875  0.526316       0.0   0.0       1.0         0.0       1.0         0.0        1.0           0.0          0.0         0.0        1.0           0.0          0.0         1.0          0.0               0.0             0.0            0.0           0.0        1.0      1.0            0.0         1.0        1.0          0.0          0.0\n",
            "\n",
            "\n",
            " Variance \n",
            "Top 5 Variances:\n",
            "famsup_yes : 0.23730812369812393\n",
            "Fjob_other : 0.24756289056241118\n",
            "paid_yes : 0.24825508732574794\n",
            "sex_M : 0.24929338247075764\n",
            "activities_yes : 0.24992148694119665\n",
            "\n",
            "\n",
            "--- Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ---\n",
            "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix: (41, 41)\n",
            "                        age      Medu      Fedu  traveltime  studytime  failures    famrel  freetime     goout      Dalc      Walc    health  absences        G1        G2  school_MS     sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  Fjob_teacher  reason_home  reason_other  reason_reputation  guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  activities_yes  nursery_yes  higher_yes  internet_yes  romantic_yes\n",
            "age                1.000000 -0.163658 -0.163438    0.070641  -0.004140  0.243665  0.053940  0.016434  0.126964  0.131125  0.117276 -0.062187  0.175230 -0.064081 -0.143474   0.377610 -0.028606  -0.146722     0.037847   0.029598    -0.082660    0.032488      -0.007732     -0.058256    -0.109842    0.007682       0.042963     -0.069993     0.018283      0.027172          -0.031932        -0.133558        0.398396      -0.251811   -0.140609 -0.035933       -0.103063    -0.086632   -0.209081     -0.112094      0.164669\n",
            "Medu              -0.163658  1.000000  0.623455   -0.171639   0.064944 -0.236680 -0.003914  0.030891  0.064094  0.019834 -0.047123 -0.046878  0.100285  0.205341  0.215527  -0.133333  0.078228   0.138804    -0.043068  -0.123565     0.251973   -0.235176       0.051764      0.454911     0.083398   -0.109887      -0.031834      0.259836    -0.008708      0.008229           0.106487         0.112375       -0.118635      -0.036029    0.183727  0.159700        0.108277     0.193263    0.168845      0.201463      0.039681\n",
            "Fedu              -0.163438  0.623455  1.000000   -0.158194  -0.009175 -0.250408 -0.001370 -0.012846  0.043105  0.002386 -0.012631  0.014742  0.024473  0.190270  0.164893  -0.079807  0.034878   0.072178    -0.058879  -0.088730     0.118501   -0.197077       0.028032      0.294526     0.163216   -0.253605       0.021306      0.347203    -0.009618     -0.014364           0.048724        -0.047265       -0.091250       0.037530    0.185496  0.086981        0.112643     0.157177    0.174566      0.127507      0.015602\n",
            "traveltime         0.070641 -0.171639 -0.158194    1.000000  -0.100909  0.092239 -0.016808 -0.017025  0.028540  0.138325  0.134116  0.007501 -0.012944 -0.093040 -0.153198   0.242308  0.059722  -0.328096     0.063493   0.028265    -0.106708    0.036544      -0.050943     -0.051235    -0.088277    0.093214      -0.030232      0.014004    -0.080027     -0.001662          -0.033322        -0.057669        0.048758      -0.009246   -0.003286 -0.066420       -0.007766    -0.033338   -0.083508     -0.111302      0.021962\n",
            "studytime         -0.004140  0.064944 -0.009175   -0.100909   1.000000 -0.173563  0.039731 -0.143198 -0.063904 -0.196019 -0.253785 -0.075616 -0.062700  0.160612  0.135880  -0.090681 -0.306268  -0.020912    -0.073595   0.024294    -0.012977   -0.006288      -0.004476     -0.009009     0.121107   -0.040619       0.007162     -0.058225    -0.039620     -0.107842           0.193342        -0.024031        0.031724       0.037763    0.145228  0.167220        0.089877     0.081325    0.175081      0.059422      0.053285\n",
            "failures           0.243665 -0.236680 -0.250408    0.092239  -0.173563  1.000000 -0.044337  0.091987  0.124561  0.136047  0.141962  0.065827  0.063726 -0.354718 -0.355896   0.059804  0.044436  -0.078578    -0.015769  -0.003339    -0.040859    0.006268       0.105430     -0.157771    -0.016593   -0.037792       0.082706     -0.074377     0.042511     -0.012203          -0.070127        -0.134498        0.291157      -0.000437   -0.055075 -0.188039       -0.069341    -0.100734   -0.300316     -0.063451      0.093137\n",
            "famrel             0.053940 -0.003914 -0.001370   -0.016808   0.039731 -0.044337  1.000000  0.150701  0.064568 -0.077594 -0.113397  0.094056 -0.044354  0.022168 -0.018281  -0.047926  0.058971   0.014258    -0.022776   0.025179    -0.061548    0.022736       0.056255     -0.022122    -0.013521    0.017534       0.051461     -0.069204    -0.012201     -0.019595          -0.000972        -0.010983        0.049548      -0.001345   -0.020436  0.000460        0.040687    -0.003581    0.024319      0.032768     -0.063816\n",
            "freetime           0.016434  0.030891 -0.012846   -0.017025  -0.143198  0.091987  0.150701  1.000000  0.285019  0.209001  0.147822  0.075733 -0.058078  0.012613 -0.013777   0.032988  0.238744   0.034878     0.017695   0.038717    -0.009094   -0.011640       0.021672      0.088508    -0.063738    0.040338      -0.051570      0.001675    -0.083249      0.039891          -0.050101        -0.028979        0.069438      -0.045465    0.010538 -0.064253        0.089728    -0.024696   -0.061244      0.051286     -0.011182\n",
            "goout              0.126964  0.064094  0.043105    0.028540  -0.063904  0.124561  0.064568  0.285019  1.000000  0.266994  0.420386 -0.009577  0.044302 -0.149104 -0.162250  -0.007152  0.075897   0.068835     0.023064   0.003459     0.059252   -0.001660       0.014455     -0.021319    -0.021394    0.047488      -0.010554     -0.018830    -0.004410     -0.015182          -0.038297         0.080236       -0.012380      -0.037698   -0.015631  0.010493        0.046088     0.004612   -0.039700      0.074370      0.007870\n",
            "Dalc               0.131125  0.019834  0.002386    0.138325  -0.196019  0.136047 -0.077594  0.209001  0.266994  1.000000  0.647544  0.077180  0.111908 -0.094159 -0.064120   0.114209  0.268171  -0.093494     0.101521  -0.030590    -0.074620    0.018872       0.028880      0.041015    -0.036273   -0.076526       0.111475      0.000552     0.022726      0.155076          -0.119213        -0.051224        0.037626      -0.021485   -0.031575  0.062465       -0.066508    -0.084849   -0.069828      0.036210      0.015121\n",
            "Walc               0.117276 -0.047123 -0.012631    0.134116  -0.253785  0.141962 -0.113397  0.147822  0.420386  0.647544  1.000000  0.092476  0.136291 -0.126179 -0.084927   0.065087  0.274194  -0.101126     0.103425   0.006045     0.021763   -0.024857      -0.004426      0.006194    -0.068333    0.050724       0.090568     -0.093898     0.005574      0.092452          -0.082733         0.006471       -0.045563      -0.087152   -0.086688  0.060454       -0.037477    -0.099534   -0.100340      0.011687     -0.010141\n",
            "health            -0.062187 -0.046878  0.014742    0.007501  -0.075616  0.065827  0.094056  0.075733 -0.009577  0.077180  0.092476  1.000000 -0.029937 -0.073172 -0.097720  -0.042651  0.143588  -0.040355    -0.028992   0.022307     0.046475   -0.034914       0.082612     -0.005960     0.078859   -0.001141      -0.038703      0.020423    -0.001766      0.006592          -0.145337        -0.025096       -0.045049      -0.034124    0.029297 -0.078132        0.023923    -0.018475   -0.015895     -0.080189      0.026342\n",
            "absences           0.175230  0.100285  0.024473   -0.012944  -0.062700  0.063726 -0.044354 -0.058078  0.044302  0.111908  0.136291 -0.029937  1.000000 -0.031003 -0.031777  -0.088480 -0.066962  -0.027874     0.035783  -0.134937    -0.044156    0.043005       0.023076      0.001688    -0.004191    0.016664       0.012907     -0.024964     0.111061     -0.003873           0.070671         0.023638        0.140826       0.022526    0.024353  0.007435       -0.013610     0.019155   -0.056085      0.101701      0.153384\n",
            "G1                -0.064081  0.205341  0.190270   -0.093040   0.160612 -0.354718  0.022168  0.012613 -0.149104 -0.094159 -0.126179 -0.073172 -0.031003  1.000000  0.852118  -0.025731  0.091839   0.069704     0.071445  -0.016868     0.120074   -0.164421       0.085906      0.078294     0.035303   -0.113924      -0.015093      0.168782    -0.017198     -0.007215           0.099522        -0.011767       -0.031022      -0.212607   -0.084569  0.039079        0.057010     0.069263    0.178264      0.071619     -0.037188\n",
            "G2                -0.143474  0.215527  0.164893   -0.153198   0.135880 -0.355896 -0.018281 -0.013777 -0.162250 -0.064120 -0.084927 -0.097720 -0.031777  0.852118  1.000000  -0.050086  0.091099   0.126037     0.081223  -0.041382     0.133893   -0.106426       0.078995      0.056343     0.039259   -0.089285       0.028117      0.096364     0.004798      0.042846           0.087008        -0.014442       -0.073712      -0.117385   -0.059166  0.105198        0.050552     0.068146    0.179129      0.119439     -0.111774\n",
            "school_MS          0.377610 -0.133333 -0.079807    0.242308  -0.090681  0.059804 -0.047926  0.032988 -0.007152  0.114209  0.065087 -0.042651 -0.088480 -0.025731 -0.050086   1.000000 -0.012286  -0.279797     0.064866   0.045923    -0.055139    0.042497      -0.053837     -0.016823    -0.079329   -0.067746       0.106639     -0.041674    -0.029905      0.131836          -0.129128        -0.064783        0.065759      -0.139789   -0.164967 -0.017083       -0.116946    -0.089277   -0.024150     -0.133578      0.060700\n",
            "sex_M             -0.028606  0.078228  0.034878    0.059722  -0.306268  0.044436  0.058971  0.238744  0.075897  0.268171  0.274194  0.143588 -0.066962  0.091839  0.091099  -0.012286  1.000000  -0.028504     0.089862   0.023443    -0.019817    0.002626       0.002748      0.165344    -0.061306    0.033307       0.005083      0.044147     0.049883      0.016859          -0.111434        -0.013642       -0.058525      -0.138271   -0.151623 -0.129126        0.099833    -0.008203   -0.151056      0.044113     -0.102023\n",
            "address_U         -0.146722  0.138804  0.072178   -0.328096  -0.020912 -0.078578  0.014258  0.034878  0.068835 -0.093494 -0.101126 -0.040355 -0.027874  0.069704  0.126037  -0.279797 -0.028504   1.000000     0.072472  -0.042572     0.099233   -0.032856       0.068549      0.033030     0.058641   -0.056929       0.023404      0.010748     0.153580     -0.041850          -0.077227        -0.081376        0.025177       0.024712    0.023903  0.052800       -0.051360     0.059589    0.042854      0.216842      0.005257\n",
            "famsize_LE3        0.037847 -0.043068 -0.058879    0.063493  -0.073595 -0.015769 -0.022776  0.017695  0.023064  0.101521  0.103425 -0.028992  0.035783  0.071445  0.081223   0.064866  0.089862   0.072472     1.000000  -0.149612     0.003732   -0.066398       0.041657      0.067259     0.021568    0.015408      -0.012871     -0.072183    -0.005728     -0.026982          -0.003842         0.026727       -0.025298      -0.028642   -0.112893 -0.013882       -0.000113     0.102088   -0.005806      0.000720      0.034395\n",
            "Pstatus_T          0.029598 -0.123565 -0.088730    0.028265   0.024294 -0.003339  0.025179  0.038717  0.003459 -0.030590  0.006045  0.022307 -0.134937 -0.016868 -0.041382   0.045923  0.023443  -0.042572    -0.149612   1.000000     0.015659   -0.023641      -0.062556      0.023927    -0.045041   -0.007940       0.083496     -0.063327    -0.031310      0.021247          -0.001903        -0.065813       -0.051061      -0.042238    0.019067  0.046435        0.097352    -0.090615   -0.040735      0.070074     -0.040451\n",
            "Mjob_health       -0.082660  0.251973  0.118501   -0.106708  -0.012977 -0.040859 -0.061548 -0.009094  0.059252 -0.074620  0.021763  0.046475 -0.044156  0.120074  0.133893  -0.055139 -0.019817   0.099233     0.003732   0.015659     1.000000   -0.228654      -0.182269     -0.127317     0.192628   -0.030450       0.008947     -0.051779    -0.007719      0.090990           0.080952        -0.048816       -0.024957      -0.064330    0.095788  0.080076        0.012616     0.066446    0.070874      0.089064      0.031343\n",
            "Mjob_other         0.032488 -0.235176 -0.197077    0.036544  -0.006288  0.006268  0.022736 -0.011640 -0.001660  0.018872 -0.024857 -0.034914  0.043005 -0.164421 -0.106426   0.042497  0.002626  -0.032856    -0.066398  -0.023641    -0.228654    1.000000      -0.442507     -0.309095    -0.112126    0.281851      -0.183657     -0.088168     0.095650     -0.089058           0.030130        -0.096649        0.030545       0.044043   -0.123493 -0.112524       -0.092480    -0.092740    0.003356     -0.048733      0.043476\n",
            "Mjob_services     -0.007732  0.051764  0.028032   -0.050943  -0.004476  0.105430  0.056255  0.021672  0.014455  0.028880 -0.004426  0.082612  0.023076  0.085906  0.078995  -0.053837  0.002748   0.068549     0.041657  -0.062556    -0.182269   -0.442507       1.000000     -0.246392    -0.019179   -0.169024       0.180310      0.009683    -0.044154      0.032310           0.034201        -0.014818        0.013857       0.046450    0.046119  0.009288        0.041377     0.030299    0.005660      0.080531     -0.041810\n",
            "Mjob_teacher      -0.058256  0.454911  0.294526   -0.051235  -0.009009 -0.157771 -0.022122  0.088508 -0.021319  0.041015  0.006194 -0.005960  0.001688  0.078294  0.056343  -0.016823  0.165344   0.033030     0.067259   0.023927    -0.127317   -0.309095      -0.246392      1.000000     0.046540   -0.156167       0.042985      0.212309    -0.000081     -0.031962          -0.039145         0.153480       -0.070746      -0.095745    0.065572  0.135270        0.107108     0.104411    0.063183      0.128292     -0.051288\n",
            "Fjob_health       -0.109842  0.083398  0.163216   -0.088277   0.121107 -0.016593 -0.013521 -0.063738 -0.021394 -0.036273 -0.068333  0.078859 -0.004191  0.035303  0.039259  -0.079329 -0.061306   0.058641     0.021568  -0.045041     0.192628   -0.112126      -0.019179      0.046540     1.000000   -0.241260      -0.136605     -0.061507    -0.053422      0.057341           0.115828        -0.064122       -0.020387       0.096873    0.098983  0.067047       -0.003873     0.050847    0.050462     -0.032293     -0.000391\n",
            "Fjob_other         0.007682 -0.109887 -0.253605    0.093214  -0.040619 -0.037792  0.017534  0.040338  0.047488 -0.076526  0.050724 -0.001141  0.016664 -0.113924 -0.089285  -0.067746  0.033307  -0.056929     0.015408  -0.007940    -0.030450    0.281851      -0.169024     -0.156167    -0.241260    1.000000      -0.690275     -0.310798     0.024121     -0.119815           0.026680         0.154429        0.026485      -0.045790   -0.020334  0.015977       -0.055193    -0.081931    0.046121     -0.064674     -0.059505\n",
            "Fjob_services      0.042963 -0.031834  0.021306   -0.030232   0.007162  0.082706  0.051461 -0.051570 -0.010554  0.111475  0.090568 -0.038703  0.012907 -0.015093  0.028117   0.106639  0.005083   0.023404    -0.012871   0.083496     0.008947   -0.183657       0.180310      0.042985    -0.136605   -0.690275       1.000000     -0.175979    -0.007943      0.075999          -0.044703        -0.130637        0.020799      -0.039163   -0.069429  0.001545        0.073416    -0.003320   -0.086822      0.098841      0.022762\n",
            "Fjob_teacher      -0.069993  0.259836  0.347203    0.014004  -0.058225 -0.074377 -0.069204  0.001675 -0.018830  0.000552 -0.093898  0.020423 -0.024964  0.168782  0.096364  -0.041674  0.044147   0.010748    -0.072183  -0.063327    -0.051779   -0.088168       0.009683      0.212309    -0.061507   -0.310798      -0.175979      1.000000    -0.021770      0.079490          -0.037547        -0.000904       -0.083576       0.065294    0.064417 -0.064065       -0.014697     0.046804    0.020735     -0.030037      0.026933\n",
            "reason_home        0.018283 -0.008708 -0.009618   -0.080027  -0.039620  0.042511 -0.012201 -0.083249 -0.004410  0.022726  0.005574 -0.001766  0.111061 -0.017198  0.004798  -0.029905  0.049883   0.153580    -0.005728  -0.031310    -0.007719    0.095650      -0.044154     -0.000081    -0.053422    0.024121      -0.007943     -0.021770     1.000000     -0.195494          -0.371472        -0.065389        0.086550      -0.001240    0.002561  0.080175       -0.050595     0.018964    0.065072      0.048775      0.030914\n",
            "reason_other       0.027172  0.008229 -0.014364   -0.001662  -0.107842 -0.012203 -0.019595  0.039891 -0.015182  0.155076  0.092452  0.006592 -0.003873 -0.007215  0.042846   0.131836  0.016859  -0.041850    -0.026982   0.021247     0.090990   -0.089058       0.032310     -0.031962     0.057341   -0.119815       0.075999      0.079490    -0.195494      1.000000          -0.190546         0.021304       -0.029545      -0.017001   -0.073234  0.061857       -0.023208    -0.013458   -0.127472     -0.023221      0.074026\n",
            "reason_reputation -0.031932  0.106487  0.048724   -0.033322   0.193342 -0.070127 -0.000972 -0.050101 -0.038297 -0.119213 -0.082733 -0.145337  0.070671  0.099522  0.087008  -0.129128 -0.111434  -0.077227    -0.003842  -0.001903     0.080952    0.030130       0.034201     -0.039145     0.115828    0.026680      -0.044703     -0.037547    -0.371472     -0.190546           1.000000        -0.019469        0.010368       0.024661    0.113767  0.067699        0.132624     0.050127    0.086686      0.039084     -0.025374\n",
            "guardian_mother   -0.133558  0.112375 -0.047265   -0.057669  -0.024031 -0.134498 -0.010983 -0.028979  0.080236 -0.051224  0.006471 -0.025096  0.023638 -0.011767 -0.014442  -0.064783 -0.013642  -0.081376     0.026727  -0.065813    -0.048816   -0.096649      -0.014818      0.153480    -0.064122    0.154429      -0.130637     -0.000904    -0.065389      0.021304          -0.019469         1.000000       -0.444143      -0.004054   -0.014124  0.064926        0.022809     0.108332   -0.004429     -0.020340     -0.037525\n",
            "guardian_other     0.398396 -0.118635 -0.091250    0.048758   0.031724  0.291157  0.049548  0.069438 -0.012380  0.037626 -0.045563 -0.045049  0.140826 -0.031022 -0.073712   0.065759 -0.058525   0.025177    -0.025298  -0.051061    -0.024957    0.030545       0.013857     -0.070746    -0.020387    0.026485       0.020799     -0.083576     0.086550     -0.029545           0.010368        -0.444143        1.000000      -0.058982    0.007522 -0.012352       -0.042382    -0.170929   -0.016071     -0.041116      0.124045\n",
            "schoolsup_yes     -0.251811 -0.036029  0.037530   -0.009246   0.037763 -0.000437 -0.001345 -0.045465 -0.037698 -0.021485 -0.087152 -0.034124  0.022526 -0.212607 -0.117385  -0.139789 -0.138271   0.024712    -0.028642  -0.042238    -0.064330    0.044043       0.046450     -0.095745     0.096873   -0.045790      -0.039163      0.065294    -0.001240     -0.017001           0.024661        -0.004054       -0.058982       1.000000    0.104681 -0.020753        0.046032     0.045967    0.054486     -0.009683     -0.080716\n",
            "famsup_yes        -0.140609  0.183727  0.185496   -0.003286   0.145228 -0.055075 -0.020436  0.010538 -0.015631 -0.031575 -0.086688  0.029297  0.024353 -0.084569 -0.059166  -0.164967 -0.151623   0.023903    -0.112893   0.019067     0.095788   -0.123493       0.046119      0.065572     0.098983   -0.020334      -0.069429      0.064417     0.002561     -0.073234           0.113767        -0.014124        0.007522       0.104681    1.000000  0.293184       -0.001500     0.059536    0.100815      0.103581      0.012440\n",
            "paid_yes          -0.035933  0.159700  0.086981   -0.066420   0.167220 -0.188039  0.000460 -0.064253  0.010493  0.062465  0.060454 -0.078132  0.007435  0.039079  0.105198  -0.017083 -0.129126   0.052800    -0.013882   0.046435     0.080076   -0.112524       0.009288      0.135270     0.067047    0.015977       0.001545     -0.064065     0.080175      0.061857           0.067699         0.064926       -0.012352      -0.020753    0.293184  1.000000       -0.021382     0.102143    0.189214      0.153132      0.005536\n",
            "activities_yes    -0.103063  0.108277  0.112643   -0.007766   0.089877 -0.069341  0.040687  0.089728  0.046088 -0.066508 -0.037477  0.023923 -0.013610  0.057010  0.050552  -0.116946  0.099833  -0.051360    -0.000113   0.097352     0.012616   -0.092480       0.041377      0.107108    -0.003873   -0.055193       0.073416     -0.014697    -0.050595     -0.023208           0.132624         0.022809       -0.042382       0.046032   -0.001500 -0.021382        1.000000     0.002731    0.096484      0.048663      0.019651\n",
            "nursery_yes       -0.086632  0.193263  0.157177   -0.033338   0.081325 -0.100734 -0.003581 -0.024696  0.004612 -0.084849 -0.099534 -0.018475  0.019155  0.069263  0.068146  -0.089277 -0.008203   0.059589     0.102088  -0.090615     0.066446   -0.092740       0.030299      0.104411     0.050847   -0.081931      -0.003320      0.046804     0.018964     -0.013458           0.050127         0.108332       -0.170929       0.045967    0.059536  0.102143        0.002731     1.000000    0.054303      0.007830      0.027495\n",
            "higher_yes        -0.209081  0.168845  0.174566   -0.083508   0.175081 -0.300316  0.024319 -0.061244 -0.039700 -0.069828 -0.100340 -0.015895 -0.056085  0.178264  0.179129  -0.024150 -0.151056   0.042854    -0.005806  -0.040735     0.070874    0.003356       0.005660      0.063183     0.050462    0.046121      -0.086822      0.020735     0.065072     -0.127472           0.086686        -0.004429       -0.016071       0.054486    0.100815  0.189214        0.096484     0.054303    1.000000      0.020374     -0.105664\n",
            "internet_yes      -0.112094  0.201463  0.127507   -0.111302   0.059422 -0.063451  0.032768  0.051286  0.074370  0.036210  0.011687 -0.080189  0.101701  0.071619  0.119439  -0.133578  0.044113   0.216842     0.000720   0.070074     0.089064   -0.048733       0.080531      0.128292    -0.032293   -0.064674       0.098841     -0.030037     0.048775     -0.023221           0.039084        -0.020340       -0.041116      -0.009683    0.103581  0.153132        0.048663     0.007830    0.020374      1.000000      0.087122\n",
            "romantic_yes       0.164669  0.039681  0.015602    0.021962   0.053285  0.093137 -0.063816 -0.011182  0.007870  0.015121 -0.010141  0.026342  0.153384 -0.037188 -0.111774   0.060700 -0.102023   0.005257     0.034395  -0.040451     0.031343    0.043476      -0.041810     -0.051288    -0.000391   -0.059505       0.022762      0.026933     0.030914      0.074026          -0.025374        -0.037525        0.124045      -0.080716    0.012440  0.005536        0.019651     0.027495   -0.105664      0.087122      1.000000\n",
            "\n",
            "\n",
            "--- üî¥ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Threshold: > 0.90) üî¥ ---\n",
            "\n",
            "‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå ( 0.9 )\n",
            "\n",
            "--- Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---\n",
            "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: (41, 41)\n",
            "                        age      Medu      Fedu  traveltime  studytime  failures    famrel  freetime     goout      Dalc      Walc    health  absences        G1        G2  school_MS     sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  Fjob_teacher  reason_home  reason_other  reason_reputation  guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  activities_yes  nursery_yes  higher_yes  internet_yes  romantic_yes\n",
            "age                1.000000 -0.163658 -0.163438    0.070641  -0.004140  0.243665  0.053940  0.016434  0.126964  0.131125  0.117276 -0.062187  0.175230 -0.064081 -0.143474   0.377610 -0.028606  -0.146722     0.037847   0.029598    -0.082660    0.032488      -0.007732     -0.058256    -0.109842    0.007682       0.042963     -0.069993     0.018283      0.027172          -0.031932        -0.133558        0.398396      -0.251811   -0.140609 -0.035933       -0.103063    -0.086632   -0.209081     -0.112094      0.164669\n",
            "Medu              -0.163658  1.000000  0.623455   -0.171639   0.064944 -0.236680 -0.003914  0.030891  0.064094  0.019834 -0.047123 -0.046878  0.100285  0.205341  0.215527  -0.133333  0.078228   0.138804    -0.043068  -0.123565     0.251973   -0.235176       0.051764      0.454911     0.083398   -0.109887      -0.031834      0.259836    -0.008708      0.008229           0.106487         0.112375       -0.118635      -0.036029    0.183727  0.159700        0.108277     0.193263    0.168845      0.201463      0.039681\n",
            "Fedu              -0.163438  0.623455  1.000000   -0.158194  -0.009175 -0.250408 -0.001370 -0.012846  0.043105  0.002386 -0.012631  0.014742  0.024473  0.190270  0.164893  -0.079807  0.034878   0.072178    -0.058879  -0.088730     0.118501   -0.197077       0.028032      0.294526     0.163216   -0.253605       0.021306      0.347203    -0.009618     -0.014364           0.048724        -0.047265       -0.091250       0.037530    0.185496  0.086981        0.112643     0.157177    0.174566      0.127507      0.015602\n",
            "traveltime         0.070641 -0.171639 -0.158194    1.000000  -0.100909  0.092239 -0.016808 -0.017025  0.028540  0.138325  0.134116  0.007501 -0.012944 -0.093040 -0.153198   0.242308  0.059722  -0.328096     0.063493   0.028265    -0.106708    0.036544      -0.050943     -0.051235    -0.088277    0.093214      -0.030232      0.014004    -0.080027     -0.001662          -0.033322        -0.057669        0.048758      -0.009246   -0.003286 -0.066420       -0.007766    -0.033338   -0.083508     -0.111302      0.021962\n",
            "studytime         -0.004140  0.064944 -0.009175   -0.100909   1.000000 -0.173563  0.039731 -0.143198 -0.063904 -0.196019 -0.253785 -0.075616 -0.062700  0.160612  0.135880  -0.090681 -0.306268  -0.020912    -0.073595   0.024294    -0.012977   -0.006288      -0.004476     -0.009009     0.121107   -0.040619       0.007162     -0.058225    -0.039620     -0.107842           0.193342        -0.024031        0.031724       0.037763    0.145228  0.167220        0.089877     0.081325    0.175081      0.059422      0.053285\n",
            "failures           0.243665 -0.236680 -0.250408    0.092239  -0.173563  1.000000 -0.044337  0.091987  0.124561  0.136047  0.141962  0.065827  0.063726 -0.354718 -0.355896   0.059804  0.044436  -0.078578    -0.015769  -0.003339    -0.040859    0.006268       0.105430     -0.157771    -0.016593   -0.037792       0.082706     -0.074377     0.042511     -0.012203          -0.070127        -0.134498        0.291157      -0.000437   -0.055075 -0.188039       -0.069341    -0.100734   -0.300316     -0.063451      0.093137\n",
            "famrel             0.053940 -0.003914 -0.001370   -0.016808   0.039731 -0.044337  1.000000  0.150701  0.064568 -0.077594 -0.113397  0.094056 -0.044354  0.022168 -0.018281  -0.047926  0.058971   0.014258    -0.022776   0.025179    -0.061548    0.022736       0.056255     -0.022122    -0.013521    0.017534       0.051461     -0.069204    -0.012201     -0.019595          -0.000972        -0.010983        0.049548      -0.001345   -0.020436  0.000460        0.040687    -0.003581    0.024319      0.032768     -0.063816\n",
            "freetime           0.016434  0.030891 -0.012846   -0.017025  -0.143198  0.091987  0.150701  1.000000  0.285019  0.209001  0.147822  0.075733 -0.058078  0.012613 -0.013777   0.032988  0.238744   0.034878     0.017695   0.038717    -0.009094   -0.011640       0.021672      0.088508    -0.063738    0.040338      -0.051570      0.001675    -0.083249      0.039891          -0.050101        -0.028979        0.069438      -0.045465    0.010538 -0.064253        0.089728    -0.024696   -0.061244      0.051286     -0.011182\n",
            "goout              0.126964  0.064094  0.043105    0.028540  -0.063904  0.124561  0.064568  0.285019  1.000000  0.266994  0.420386 -0.009577  0.044302 -0.149104 -0.162250  -0.007152  0.075897   0.068835     0.023064   0.003459     0.059252   -0.001660       0.014455     -0.021319    -0.021394    0.047488      -0.010554     -0.018830    -0.004410     -0.015182          -0.038297         0.080236       -0.012380      -0.037698   -0.015631  0.010493        0.046088     0.004612   -0.039700      0.074370      0.007870\n",
            "Dalc               0.131125  0.019834  0.002386    0.138325  -0.196019  0.136047 -0.077594  0.209001  0.266994  1.000000  0.647544  0.077180  0.111908 -0.094159 -0.064120   0.114209  0.268171  -0.093494     0.101521  -0.030590    -0.074620    0.018872       0.028880      0.041015    -0.036273   -0.076526       0.111475      0.000552     0.022726      0.155076          -0.119213        -0.051224        0.037626      -0.021485   -0.031575  0.062465       -0.066508    -0.084849   -0.069828      0.036210      0.015121\n",
            "Walc               0.117276 -0.047123 -0.012631    0.134116  -0.253785  0.141962 -0.113397  0.147822  0.420386  0.647544  1.000000  0.092476  0.136291 -0.126179 -0.084927   0.065087  0.274194  -0.101126     0.103425   0.006045     0.021763   -0.024857      -0.004426      0.006194    -0.068333    0.050724       0.090568     -0.093898     0.005574      0.092452          -0.082733         0.006471       -0.045563      -0.087152   -0.086688  0.060454       -0.037477    -0.099534   -0.100340      0.011687     -0.010141\n",
            "health            -0.062187 -0.046878  0.014742    0.007501  -0.075616  0.065827  0.094056  0.075733 -0.009577  0.077180  0.092476  1.000000 -0.029937 -0.073172 -0.097720  -0.042651  0.143588  -0.040355    -0.028992   0.022307     0.046475   -0.034914       0.082612     -0.005960     0.078859   -0.001141      -0.038703      0.020423    -0.001766      0.006592          -0.145337        -0.025096       -0.045049      -0.034124    0.029297 -0.078132        0.023923    -0.018475   -0.015895     -0.080189      0.026342\n",
            "absences           0.175230  0.100285  0.024473   -0.012944  -0.062700  0.063726 -0.044354 -0.058078  0.044302  0.111908  0.136291 -0.029937  1.000000 -0.031003 -0.031777  -0.088480 -0.066962  -0.027874     0.035783  -0.134937    -0.044156    0.043005       0.023076      0.001688    -0.004191    0.016664       0.012907     -0.024964     0.111061     -0.003873           0.070671         0.023638        0.140826       0.022526    0.024353  0.007435       -0.013610     0.019155   -0.056085      0.101701      0.153384\n",
            "G1                -0.064081  0.205341  0.190270   -0.093040   0.160612 -0.354718  0.022168  0.012613 -0.149104 -0.094159 -0.126179 -0.073172 -0.031003  1.000000  0.852118  -0.025731  0.091839   0.069704     0.071445  -0.016868     0.120074   -0.164421       0.085906      0.078294     0.035303   -0.113924      -0.015093      0.168782    -0.017198     -0.007215           0.099522        -0.011767       -0.031022      -0.212607   -0.084569  0.039079        0.057010     0.069263    0.178264      0.071619     -0.037188\n",
            "G2                -0.143474  0.215527  0.164893   -0.153198   0.135880 -0.355896 -0.018281 -0.013777 -0.162250 -0.064120 -0.084927 -0.097720 -0.031777  0.852118  1.000000  -0.050086  0.091099   0.126037     0.081223  -0.041382     0.133893   -0.106426       0.078995      0.056343     0.039259   -0.089285       0.028117      0.096364     0.004798      0.042846           0.087008        -0.014442       -0.073712      -0.117385   -0.059166  0.105198        0.050552     0.068146    0.179129      0.119439     -0.111774\n",
            "school_MS          0.377610 -0.133333 -0.079807    0.242308  -0.090681  0.059804 -0.047926  0.032988 -0.007152  0.114209  0.065087 -0.042651 -0.088480 -0.025731 -0.050086   1.000000 -0.012286  -0.279797     0.064866   0.045923    -0.055139    0.042497      -0.053837     -0.016823    -0.079329   -0.067746       0.106639     -0.041674    -0.029905      0.131836          -0.129128        -0.064783        0.065759      -0.139789   -0.164967 -0.017083       -0.116946    -0.089277   -0.024150     -0.133578      0.060700\n",
            "sex_M             -0.028606  0.078228  0.034878    0.059722  -0.306268  0.044436  0.058971  0.238744  0.075897  0.268171  0.274194  0.143588 -0.066962  0.091839  0.091099  -0.012286  1.000000  -0.028504     0.089862   0.023443    -0.019817    0.002626       0.002748      0.165344    -0.061306    0.033307       0.005083      0.044147     0.049883      0.016859          -0.111434        -0.013642       -0.058525      -0.138271   -0.151623 -0.129126        0.099833    -0.008203   -0.151056      0.044113     -0.102023\n",
            "address_U         -0.146722  0.138804  0.072178   -0.328096  -0.020912 -0.078578  0.014258  0.034878  0.068835 -0.093494 -0.101126 -0.040355 -0.027874  0.069704  0.126037  -0.279797 -0.028504   1.000000     0.072472  -0.042572     0.099233   -0.032856       0.068549      0.033030     0.058641   -0.056929       0.023404      0.010748     0.153580     -0.041850          -0.077227        -0.081376        0.025177       0.024712    0.023903  0.052800       -0.051360     0.059589    0.042854      0.216842      0.005257\n",
            "famsize_LE3        0.037847 -0.043068 -0.058879    0.063493  -0.073595 -0.015769 -0.022776  0.017695  0.023064  0.101521  0.103425 -0.028992  0.035783  0.071445  0.081223   0.064866  0.089862   0.072472     1.000000  -0.149612     0.003732   -0.066398       0.041657      0.067259     0.021568    0.015408      -0.012871     -0.072183    -0.005728     -0.026982          -0.003842         0.026727       -0.025298      -0.028642   -0.112893 -0.013882       -0.000113     0.102088   -0.005806      0.000720      0.034395\n",
            "Pstatus_T          0.029598 -0.123565 -0.088730    0.028265   0.024294 -0.003339  0.025179  0.038717  0.003459 -0.030590  0.006045  0.022307 -0.134937 -0.016868 -0.041382   0.045923  0.023443  -0.042572    -0.149612   1.000000     0.015659   -0.023641      -0.062556      0.023927    -0.045041   -0.007940       0.083496     -0.063327    -0.031310      0.021247          -0.001903        -0.065813       -0.051061      -0.042238    0.019067  0.046435        0.097352    -0.090615   -0.040735      0.070074     -0.040451\n",
            "Mjob_health       -0.082660  0.251973  0.118501   -0.106708  -0.012977 -0.040859 -0.061548 -0.009094  0.059252 -0.074620  0.021763  0.046475 -0.044156  0.120074  0.133893  -0.055139 -0.019817   0.099233     0.003732   0.015659     1.000000   -0.228654      -0.182269     -0.127317     0.192628   -0.030450       0.008947     -0.051779    -0.007719      0.090990           0.080952        -0.048816       -0.024957      -0.064330    0.095788  0.080076        0.012616     0.066446    0.070874      0.089064      0.031343\n",
            "Mjob_other         0.032488 -0.235176 -0.197077    0.036544  -0.006288  0.006268  0.022736 -0.011640 -0.001660  0.018872 -0.024857 -0.034914  0.043005 -0.164421 -0.106426   0.042497  0.002626  -0.032856    -0.066398  -0.023641    -0.228654    1.000000      -0.442507     -0.309095    -0.112126    0.281851      -0.183657     -0.088168     0.095650     -0.089058           0.030130        -0.096649        0.030545       0.044043   -0.123493 -0.112524       -0.092480    -0.092740    0.003356     -0.048733      0.043476\n",
            "Mjob_services     -0.007732  0.051764  0.028032   -0.050943  -0.004476  0.105430  0.056255  0.021672  0.014455  0.028880 -0.004426  0.082612  0.023076  0.085906  0.078995  -0.053837  0.002748   0.068549     0.041657  -0.062556    -0.182269   -0.442507       1.000000     -0.246392    -0.019179   -0.169024       0.180310      0.009683    -0.044154      0.032310           0.034201        -0.014818        0.013857       0.046450    0.046119  0.009288        0.041377     0.030299    0.005660      0.080531     -0.041810\n",
            "Mjob_teacher      -0.058256  0.454911  0.294526   -0.051235  -0.009009 -0.157771 -0.022122  0.088508 -0.021319  0.041015  0.006194 -0.005960  0.001688  0.078294  0.056343  -0.016823  0.165344   0.033030     0.067259   0.023927    -0.127317   -0.309095      -0.246392      1.000000     0.046540   -0.156167       0.042985      0.212309    -0.000081     -0.031962          -0.039145         0.153480       -0.070746      -0.095745    0.065572  0.135270        0.107108     0.104411    0.063183      0.128292     -0.051288\n",
            "Fjob_health       -0.109842  0.083398  0.163216   -0.088277   0.121107 -0.016593 -0.013521 -0.063738 -0.021394 -0.036273 -0.068333  0.078859 -0.004191  0.035303  0.039259  -0.079329 -0.061306   0.058641     0.021568  -0.045041     0.192628   -0.112126      -0.019179      0.046540     1.000000   -0.241260      -0.136605     -0.061507    -0.053422      0.057341           0.115828        -0.064122       -0.020387       0.096873    0.098983  0.067047       -0.003873     0.050847    0.050462     -0.032293     -0.000391\n",
            "Fjob_other         0.007682 -0.109887 -0.253605    0.093214  -0.040619 -0.037792  0.017534  0.040338  0.047488 -0.076526  0.050724 -0.001141  0.016664 -0.113924 -0.089285  -0.067746  0.033307  -0.056929     0.015408  -0.007940    -0.030450    0.281851      -0.169024     -0.156167    -0.241260    1.000000      -0.690275     -0.310798     0.024121     -0.119815           0.026680         0.154429        0.026485      -0.045790   -0.020334  0.015977       -0.055193    -0.081931    0.046121     -0.064674     -0.059505\n",
            "Fjob_services      0.042963 -0.031834  0.021306   -0.030232   0.007162  0.082706  0.051461 -0.051570 -0.010554  0.111475  0.090568 -0.038703  0.012907 -0.015093  0.028117   0.106639  0.005083   0.023404    -0.012871   0.083496     0.008947   -0.183657       0.180310      0.042985    -0.136605   -0.690275       1.000000     -0.175979    -0.007943      0.075999          -0.044703        -0.130637        0.020799      -0.039163   -0.069429  0.001545        0.073416    -0.003320   -0.086822      0.098841      0.022762\n",
            "Fjob_teacher      -0.069993  0.259836  0.347203    0.014004  -0.058225 -0.074377 -0.069204  0.001675 -0.018830  0.000552 -0.093898  0.020423 -0.024964  0.168782  0.096364  -0.041674  0.044147   0.010748    -0.072183  -0.063327    -0.051779   -0.088168       0.009683      0.212309    -0.061507   -0.310798      -0.175979      1.000000    -0.021770      0.079490          -0.037547        -0.000904       -0.083576       0.065294    0.064417 -0.064065       -0.014697     0.046804    0.020735     -0.030037      0.026933\n",
            "reason_home        0.018283 -0.008708 -0.009618   -0.080027  -0.039620  0.042511 -0.012201 -0.083249 -0.004410  0.022726  0.005574 -0.001766  0.111061 -0.017198  0.004798  -0.029905  0.049883   0.153580    -0.005728  -0.031310    -0.007719    0.095650      -0.044154     -0.000081    -0.053422    0.024121      -0.007943     -0.021770     1.000000     -0.195494          -0.371472        -0.065389        0.086550      -0.001240    0.002561  0.080175       -0.050595     0.018964    0.065072      0.048775      0.030914\n",
            "reason_other       0.027172  0.008229 -0.014364   -0.001662  -0.107842 -0.012203 -0.019595  0.039891 -0.015182  0.155076  0.092452  0.006592 -0.003873 -0.007215  0.042846   0.131836  0.016859  -0.041850    -0.026982   0.021247     0.090990   -0.089058       0.032310     -0.031962     0.057341   -0.119815       0.075999      0.079490    -0.195494      1.000000          -0.190546         0.021304       -0.029545      -0.017001   -0.073234  0.061857       -0.023208    -0.013458   -0.127472     -0.023221      0.074026\n",
            "reason_reputation -0.031932  0.106487  0.048724   -0.033322   0.193342 -0.070127 -0.000972 -0.050101 -0.038297 -0.119213 -0.082733 -0.145337  0.070671  0.099522  0.087008  -0.129128 -0.111434  -0.077227    -0.003842  -0.001903     0.080952    0.030130       0.034201     -0.039145     0.115828    0.026680      -0.044703     -0.037547    -0.371472     -0.190546           1.000000        -0.019469        0.010368       0.024661    0.113767  0.067699        0.132624     0.050127    0.086686      0.039084     -0.025374\n",
            "guardian_mother   -0.133558  0.112375 -0.047265   -0.057669  -0.024031 -0.134498 -0.010983 -0.028979  0.080236 -0.051224  0.006471 -0.025096  0.023638 -0.011767 -0.014442  -0.064783 -0.013642  -0.081376     0.026727  -0.065813    -0.048816   -0.096649      -0.014818      0.153480    -0.064122    0.154429      -0.130637     -0.000904    -0.065389      0.021304          -0.019469         1.000000       -0.444143      -0.004054   -0.014124  0.064926        0.022809     0.108332   -0.004429     -0.020340     -0.037525\n",
            "guardian_other     0.398396 -0.118635 -0.091250    0.048758   0.031724  0.291157  0.049548  0.069438 -0.012380  0.037626 -0.045563 -0.045049  0.140826 -0.031022 -0.073712   0.065759 -0.058525   0.025177    -0.025298  -0.051061    -0.024957    0.030545       0.013857     -0.070746    -0.020387    0.026485       0.020799     -0.083576     0.086550     -0.029545           0.010368        -0.444143        1.000000      -0.058982    0.007522 -0.012352       -0.042382    -0.170929   -0.016071     -0.041116      0.124045\n",
            "schoolsup_yes     -0.251811 -0.036029  0.037530   -0.009246   0.037763 -0.000437 -0.001345 -0.045465 -0.037698 -0.021485 -0.087152 -0.034124  0.022526 -0.212607 -0.117385  -0.139789 -0.138271   0.024712    -0.028642  -0.042238    -0.064330    0.044043       0.046450     -0.095745     0.096873   -0.045790      -0.039163      0.065294    -0.001240     -0.017001           0.024661        -0.004054       -0.058982       1.000000    0.104681 -0.020753        0.046032     0.045967    0.054486     -0.009683     -0.080716\n",
            "famsup_yes        -0.140609  0.183727  0.185496   -0.003286   0.145228 -0.055075 -0.020436  0.010538 -0.015631 -0.031575 -0.086688  0.029297  0.024353 -0.084569 -0.059166  -0.164967 -0.151623   0.023903    -0.112893   0.019067     0.095788   -0.123493       0.046119      0.065572     0.098983   -0.020334      -0.069429      0.064417     0.002561     -0.073234           0.113767        -0.014124        0.007522       0.104681    1.000000  0.293184       -0.001500     0.059536    0.100815      0.103581      0.012440\n",
            "paid_yes          -0.035933  0.159700  0.086981   -0.066420   0.167220 -0.188039  0.000460 -0.064253  0.010493  0.062465  0.060454 -0.078132  0.007435  0.039079  0.105198  -0.017083 -0.129126   0.052800    -0.013882   0.046435     0.080076   -0.112524       0.009288      0.135270     0.067047    0.015977       0.001545     -0.064065     0.080175      0.061857           0.067699         0.064926       -0.012352      -0.020753    0.293184  1.000000       -0.021382     0.102143    0.189214      0.153132      0.005536\n",
            "activities_yes    -0.103063  0.108277  0.112643   -0.007766   0.089877 -0.069341  0.040687  0.089728  0.046088 -0.066508 -0.037477  0.023923 -0.013610  0.057010  0.050552  -0.116946  0.099833  -0.051360    -0.000113   0.097352     0.012616   -0.092480       0.041377      0.107108    -0.003873   -0.055193       0.073416     -0.014697    -0.050595     -0.023208           0.132624         0.022809       -0.042382       0.046032   -0.001500 -0.021382        1.000000     0.002731    0.096484      0.048663      0.019651\n",
            "nursery_yes       -0.086632  0.193263  0.157177   -0.033338   0.081325 -0.100734 -0.003581 -0.024696  0.004612 -0.084849 -0.099534 -0.018475  0.019155  0.069263  0.068146  -0.089277 -0.008203   0.059589     0.102088  -0.090615     0.066446   -0.092740       0.030299      0.104411     0.050847   -0.081931      -0.003320      0.046804     0.018964     -0.013458           0.050127         0.108332       -0.170929       0.045967    0.059536  0.102143        0.002731     1.000000    0.054303      0.007830      0.027495\n",
            "higher_yes        -0.209081  0.168845  0.174566   -0.083508   0.175081 -0.300316  0.024319 -0.061244 -0.039700 -0.069828 -0.100340 -0.015895 -0.056085  0.178264  0.179129  -0.024150 -0.151056   0.042854    -0.005806  -0.040735     0.070874    0.003356       0.005660      0.063183     0.050462    0.046121      -0.086822      0.020735     0.065072     -0.127472           0.086686        -0.004429       -0.016071       0.054486    0.100815  0.189214        0.096484     0.054303    1.000000      0.020374     -0.105664\n",
            "internet_yes      -0.112094  0.201463  0.127507   -0.111302   0.059422 -0.063451  0.032768  0.051286  0.074370  0.036210  0.011687 -0.080189  0.101701  0.071619  0.119439  -0.133578  0.044113   0.216842     0.000720   0.070074     0.089064   -0.048733       0.080531      0.128292    -0.032293   -0.064674       0.098841     -0.030037     0.048775     -0.023221           0.039084        -0.020340       -0.041116      -0.009683    0.103581  0.153132        0.048663     0.007830    0.020374      1.000000      0.087122\n",
            "romantic_yes       0.164669  0.039681  0.015602    0.021962   0.053285  0.093137 -0.063816 -0.011182  0.007870  0.015121 -0.010141  0.026342  0.153384 -0.037188 -0.111774   0.060700 -0.102023   0.005257     0.034395  -0.040451     0.031343    0.043476      -0.041810     -0.051288    -0.000391   -0.059505       0.022762      0.026933     0.030914      0.074026          -0.025374        -0.037525        0.124045      -0.080716    0.012440  0.005536        0.019651     0.027495   -0.105664      0.087122      1.000000\n",
            "--- üèÅ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ üèÅ ---\n",
            "‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠: ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'school_MS', 'sex_M', 'address_U', 'famsize_LE3', 'Pstatus_T', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_mother', 'guardian_other', 'schoolsup_yes', 'famsup_yes', 'paid_yes', 'activities_yes', 'nursery_yes', 'higher_yes', 'internet_yes', 'romantic_yes']\n",
            "\n",
            "--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ ---\n",
            "‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'school_MS', 'sex_M', 'address_U', 'famsize_LE3', 'Pstatus_T', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_mother', 'guardian_other', 'schoolsup_yes', 'famsup_yes', 'paid_yes', 'activities_yes', 'nursery_yes', 'higher_yes', 'internet_yes', 'romantic_yes']\n",
            "Accuracy (Features Selected by Correlation/SD): 0.8739495798319328\n",
            "\n",
            "\n",
            " Features \n",
            "Accuracy (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å Features): 0.865546218487395\n",
            "\n",
            "\n",
            "Threshold = 0.03\n",
            "Accuracy: 0.8991596638655462\n",
            "\n",
            "\n",
            " Best Features (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° Variance)\n",
            "Accuracy (Top 3): 0.5294117647058824\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"step1_student_data.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1NG1zPbJTgl8GwuIAMicEZm9Fb07nCx5H\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Pandas ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏¢‡∏≤‡∏ß ‡πÜ **\n",
        "# *******************************************************************\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "# *******************************************************************\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• student-mat.csv **\n",
        "# *******************************************************************\n",
        "\n",
        "# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ã‡∏°‡∏¥‡πÇ‡∏Ñ‡∏•‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô\n",
        "df = pd.read_csv('student-mat.csv', sep=';')\n",
        "\n",
        "# 2. ‡∏ó‡∏≥ One-Hot Encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Categorical)\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Target Column (‡πÉ‡∏ä‡πâ G3 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)\n",
        "TARGET_COLUMN = 'G3'\n",
        "\n",
        "# X_df ‡∏Ñ‡∏∑‡∏≠ Features (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢)\n",
        "X_df = df_encoded.drop(TARGET_COLUMN, axis=1)\n",
        "\n",
        "# y_series ‡∏Ñ‡∏∑‡∏≠ Target (‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ G3 ‡πÄ‡∏õ‡πá‡∏ô Binary: Pass/Fail\n",
        "# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠ > 10\n",
        "y = (df_encoded[TARGET_COLUMN] > 10).astype(int).values\n",
        "\n",
        "# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô\n",
        "X = X_df.values\n",
        "feature_names = X_df.columns.tolist()\n",
        "\n",
        "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\")\n",
        "print(X_df.head())\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "\n",
        "print(\" MinMax (Manual Scaling) \")\n",
        "# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Min-Max Scaling ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á\n",
        "denominator = X_max - X_min\n",
        "# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà X_max == X_min (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå)\n",
        "X_scaled = np.where(denominator == 0, 0, (X - X_min) / denominator)\n",
        "# --- ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Scale ‡πÅ‡∏•‡πâ‡∏ß\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà Scale ‡πÅ‡∏•‡πâ‡∏ß (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\")\n",
        "print(df_scaled.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Variance \")\n",
        "variances = X_scaled.var(axis=0)\n",
        "print(\"Top 5 Variances:\")\n",
        "for i in np.argsort(variances)[-5:]:\n",
        "    print(feature_names[i], \":\", variances[i])\n",
        "print(\"\\n\")\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô **\n",
        "# *******************************************************************\n",
        "print(\"--- Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ---\")\n",
        "correlation_matrix = df_scaled.corr()\n",
        "print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix:\", correlation_matrix.shape)\n",
        "print(correlation_matrix)\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ: ‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Correlation-Based Feature Selection) **\n",
        "# *******************************************************************\n",
        "\n",
        "def select_features_by_correlation(df, threshold=0.65):\n",
        "    \"\"\"\n",
        "    ‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
        "    ‡πÇ‡∏î‡∏¢‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Standard Deviation (S.D.) ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏ß‡πâ\n",
        "    \"\"\"\n",
        "    print(\"--- üî¥ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Threshold: > %.2f) üî¥ ---\" % threshold)\n",
        "    df_current = df.copy()\n",
        "\n",
        "    round_count = 1\n",
        "\n",
        "    while True:\n",
        "        # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "        corr_matrix = df_current.corr().abs()\n",
        "\n",
        "        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏ô‡∏ß‡∏ó‡πÅ‡∏¢‡∏á‡∏°‡∏∏‡∏°)\n",
        "        np.fill_diagonal(corr_matrix.values, 0)\n",
        "\n",
        "        max_corr_value = corr_matrix.max().max()\n",
        "\n",
        "        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (Threshold)\n",
        "        if max_corr_value <= threshold:\n",
        "            print(\"\\n‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå (\", threshold, \")\")\n",
        "\n",
        "            # ** ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Correlation Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **\n",
        "            final_corr_matrix = df_current.corr()\n",
        "            print(\"\\n--- Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---\")\n",
        "            print(\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢:\", final_corr_matrix.shape)\n",
        "            print(final_corr_matrix)\n",
        "\n",
        "            break\n",
        "\n",
        "        # ‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "        r, c = np.unravel_index(corr_matrix.values.argmax(), corr_matrix.shape)\n",
        "        feature_a = corr_matrix.columns[r]\n",
        "        feature_b = corr_matrix.columns[c]\n",
        "\n",
        "        print(\"\\n--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà %d ---\" % round_count)\n",
        "\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1)\n",
        "        print(\"1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î %.4f):\" % max_corr_value)\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "        print(corr_matrix.loc[[feature_a, feature_b], [feature_a, feature_b]])\n",
        "\n",
        "        # 4. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (Standard Deviation)\n",
        "        std_a = df_current[feature_a].std()\n",
        "        std_b = df_current[feature_b].std()\n",
        "\n",
        "        print(\"\\n3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\")\n",
        "        print(\"   * **%s**: S.D. = **%.5f**\" % (feature_a, std_a))\n",
        "        print(\"   * **%s**: S.D. = **%.5f**\" % (feature_b, std_b))\n",
        "\n",
        "        # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à: ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ S.D. ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤\n",
        "        if std_a >= std_b:\n",
        "            feature_to_drop = feature_b\n",
        "            feature_to_keep = feature_a\n",
        "        else:\n",
        "            feature_to_drop = feature_a\n",
        "            feature_to_keep = feature_b\n",
        "\n",
        "        # ‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ S.D. ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á\n",
        "        df_current = df_current.drop(columns=[feature_to_drop])\n",
        "\n",
        "        # 4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à\n",
        "        print(\"4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\")\n",
        "        print(\"   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: '%s' (S.D. **%.5f**)\" % (feature_to_keep, max(std_a, std_b)))\n",
        "        print(\"   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: '%s' (S.D. %.5f)\" % (feature_to_drop, min(std_a, std_b)))\n",
        "\n",
        "        round_count += 1\n",
        "\n",
        "    print(\"--- üèÅ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ üèÅ ---\")\n",
        "\n",
        "    return df_current\n",
        "\n",
        "# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞\n",
        "df_selected = select_features_by_correlation(df_scaled, threshold=0.45)\n",
        "X_selected = df_selected.values\n",
        "feature_names_selected = df_selected.columns.tolist()\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° S.D. **\n",
        "# ----------------------------------------------------\n",
        "# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÉ‡∏ä‡πâ df_selected.std() ‡πÅ‡∏ó‡∏ô np.sqrt(df_selected.var())\n",
        "final_stds = df_selected.std(axis=0)\n",
        "\n",
        "result_summary = pd.DataFrame({\n",
        "    'Feature': feature_names_selected,\n",
        "    'Standard Deviation (S.D.)': final_stds.values\n",
        "}).sort_values(by='Standard Deviation (S.D.)', ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"\\n--- üèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° S.D.) üèÜ ---\")\n",
        "print(\"‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: %d ‡∏ï‡∏±‡∏ß\" % len(feature_names_selected))\n",
        "print(result_summary)\n",
        "print(\"\\n\")\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
        "X_train, X_test, y_train, y_test_sel = train_test_split(X_selected, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_sel = model.predict(X_test)\n",
        "acc_sel = accuracy_score(y_test_sel, y_pred_sel)\n",
        "print(\"Accuracy (Features Selected by Correlation/SD):\", acc_sel)\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "\n",
        "print(\" Features \")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_1 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å Features):\", acc_1)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Threshold = 0.03\")\n",
        "selector = VarianceThreshold(threshold=0.03)\n",
        "X_new_2 = selector.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new_2, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_2 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc_2)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Best Features (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° Variance)\")\n",
        "# ‡πÉ‡∏ä‡πâ Variances ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
        "sorted_idx = np.argsort(variances)\n",
        "top3_idx = sorted_idx[-3:]\n",
        "X_best3 = X_scaled[:, top3_idx]\n",
        "X_train, X_test, y_train, y_test_3, = train_test_split(X_best3, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_3 = model.predict(X_test)\n",
        "acc_3 = accuracy_score(y_test_3, y_pred_3)\n",
        "print(\"Accuracy (Top 3):\", acc_3)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3YAMAY1_5Mt",
        "outputId": "e9179862-114b-40c0-fabd-49105b0c0f70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\n",
            "   age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  goout  Dalc  Walc  health  absences  G1  G2  school_MS  sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  Fjob_teacher  reason_home  reason_other  reason_reputation  guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  activities_yes  nursery_yes  higher_yes  internet_yes  romantic_yes\n",
            "0   18     4     4           2          2         0       4         3      4     1     1       3         6   5   6      False  False       True        False      False        False       False          False         False        False       False          False          True        False         False              False             True           False           True       False     False           False         True        True         False         False\n",
            "1   17     1     1           1          2         0       5         3      3     1     1       3         4   5   5      False  False       True        False       True        False       False          False         False        False        True          False         False        False         False              False            False           False          False        True     False           False        False        True          True         False\n",
            "2   15     1     1           1          2         3       4         3      2     2     3       3        10   7   8      False  False       True         True       True        False       False          False         False        False        True          False         False        False          True              False             True           False           True       False      True           False         True        True          True         False\n",
            "3   15     4     2           1          3         0       3         2      2     1     1       5         2  15  14      False  False       True        False       True         True       False          False         False        False       False           True         False         True         False              False             True           False          False        True      True            True         True        True          True          True\n",
            "4   16     3     3           1          2         0       4         3      2     1     2       5         4   6  10      False  False       True        False       True        False        True          False         False        False        True          False         False         True         False              False            False           False          False        True      True           False         True        True         False         False\n",
            "\n",
            "\n",
            " MinMax (Manual Scaling) \n",
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Features ‡∏ó‡∏µ‡πà Scale ‡πÅ‡∏•‡πâ‡∏ß (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)\n",
            "        age  Medu  Fedu traveltime studytime failures famrel freetime goout  Dalc  Walc health  absences      G1        G2 school_MS sex_M address_U famsize_LE3 Pstatus_T Mjob_health Mjob_other Mjob_services Mjob_teacher Fjob_health Fjob_other Fjob_services Fjob_teacher reason_home reason_other reason_reputation guardian_mother guardian_other schoolsup_yes famsup_yes paid_yes activities_yes nursery_yes higher_yes internet_yes romantic_yes\n",
            "0  0.428571   1.0   1.0   0.333333  0.333333      0.0   0.75      0.5  0.75   0.0   0.0    0.5      0.08   0.125  0.315789       0.0   0.0       1.0         0.0       0.0         0.0        0.0           0.0          0.0         0.0        0.0           0.0          1.0         0.0          0.0               0.0             1.0            0.0           1.0        0.0      0.0            0.0         1.0        1.0          0.0          0.0\n",
            "1  0.285714  0.25  0.25        0.0  0.333333      0.0    1.0      0.5   0.5   0.0   0.0    0.5  0.053333   0.125  0.263158       0.0   0.0       1.0         0.0       1.0         0.0        0.0           0.0          0.0         0.0        1.0           0.0          0.0         0.0          0.0               0.0             0.0            0.0           0.0        1.0      0.0            0.0         0.0        1.0          1.0          0.0\n",
            "2       0.0  0.25  0.25        0.0  0.333333      1.0   0.75      0.5  0.25  0.25   0.5    0.5  0.133333    0.25  0.421053       0.0   0.0       1.0         1.0       1.0         0.0        0.0           0.0          0.0         0.0        1.0           0.0          0.0         0.0          1.0               0.0             1.0            0.0           1.0        0.0      1.0            0.0         1.0        1.0          1.0          0.0\n",
            "3       0.0   1.0   0.5        0.0  0.666667      0.0    0.5     0.25  0.25   0.0   0.0    1.0  0.026667    0.75  0.736842       0.0   0.0       1.0         0.0       1.0         1.0        0.0           0.0          0.0         0.0        0.0           1.0          0.0         1.0          0.0               0.0             1.0            0.0           0.0        1.0      1.0            1.0         1.0        1.0          1.0          1.0\n",
            "4  0.142857  0.75  0.75        0.0  0.333333      0.0   0.75      0.5  0.25   0.0  0.25    1.0  0.053333  0.1875  0.526316       0.0   0.0       1.0         0.0       1.0         0.0        1.0           0.0          0.0         0.0        1.0           0.0          0.0         1.0          0.0               0.0             0.0            0.0           0.0        1.0      1.0            0.0         1.0        1.0          0.0          0.0\n",
            "\n",
            "\n",
            " Variance \n",
            "Top 5 Variances:\n",
            "famsup_yes : 0.23730812369812393\n",
            "Fjob_other : 0.24756289056241118\n",
            "paid_yes : 0.24825508732574794\n",
            "sex_M : 0.24929338247075764\n",
            "activities_yes : 0.24992148694119665\n",
            "\n",
            "\n",
            "--- Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ---\n",
            "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix: (41, 41)\n",
            "                        age      Medu      Fedu  traveltime  studytime  failures    famrel  freetime     goout      Dalc      Walc    health  absences        G1        G2  school_MS     sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  Fjob_teacher  reason_home  reason_other  reason_reputation  guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  activities_yes  nursery_yes  higher_yes  internet_yes  romantic_yes\n",
            "age                1.000000 -0.163658 -0.163438    0.070641  -0.004140  0.243665  0.053940  0.016434  0.126964  0.131125  0.117276 -0.062187  0.175230 -0.064081 -0.143474   0.377610 -0.028606  -0.146722     0.037847   0.029598    -0.082660    0.032488      -0.007732     -0.058256    -0.109842    0.007682       0.042963     -0.069993     0.018283      0.027172          -0.031932        -0.133558        0.398396      -0.251811   -0.140609 -0.035933       -0.103063    -0.086632   -0.209081     -0.112094      0.164669\n",
            "Medu              -0.163658  1.000000  0.623455   -0.171639   0.064944 -0.236680 -0.003914  0.030891  0.064094  0.019834 -0.047123 -0.046878  0.100285  0.205341  0.215527  -0.133333  0.078228   0.138804    -0.043068  -0.123565     0.251973   -0.235176       0.051764      0.454911     0.083398   -0.109887      -0.031834      0.259836    -0.008708      0.008229           0.106487         0.112375       -0.118635      -0.036029    0.183727  0.159700        0.108277     0.193263    0.168845      0.201463      0.039681\n",
            "Fedu              -0.163438  0.623455  1.000000   -0.158194  -0.009175 -0.250408 -0.001370 -0.012846  0.043105  0.002386 -0.012631  0.014742  0.024473  0.190270  0.164893  -0.079807  0.034878   0.072178    -0.058879  -0.088730     0.118501   -0.197077       0.028032      0.294526     0.163216   -0.253605       0.021306      0.347203    -0.009618     -0.014364           0.048724        -0.047265       -0.091250       0.037530    0.185496  0.086981        0.112643     0.157177    0.174566      0.127507      0.015602\n",
            "traveltime         0.070641 -0.171639 -0.158194    1.000000  -0.100909  0.092239 -0.016808 -0.017025  0.028540  0.138325  0.134116  0.007501 -0.012944 -0.093040 -0.153198   0.242308  0.059722  -0.328096     0.063493   0.028265    -0.106708    0.036544      -0.050943     -0.051235    -0.088277    0.093214      -0.030232      0.014004    -0.080027     -0.001662          -0.033322        -0.057669        0.048758      -0.009246   -0.003286 -0.066420       -0.007766    -0.033338   -0.083508     -0.111302      0.021962\n",
            "studytime         -0.004140  0.064944 -0.009175   -0.100909   1.000000 -0.173563  0.039731 -0.143198 -0.063904 -0.196019 -0.253785 -0.075616 -0.062700  0.160612  0.135880  -0.090681 -0.306268  -0.020912    -0.073595   0.024294    -0.012977   -0.006288      -0.004476     -0.009009     0.121107   -0.040619       0.007162     -0.058225    -0.039620     -0.107842           0.193342        -0.024031        0.031724       0.037763    0.145228  0.167220        0.089877     0.081325    0.175081      0.059422      0.053285\n",
            "failures           0.243665 -0.236680 -0.250408    0.092239  -0.173563  1.000000 -0.044337  0.091987  0.124561  0.136047  0.141962  0.065827  0.063726 -0.354718 -0.355896   0.059804  0.044436  -0.078578    -0.015769  -0.003339    -0.040859    0.006268       0.105430     -0.157771    -0.016593   -0.037792       0.082706     -0.074377     0.042511     -0.012203          -0.070127        -0.134498        0.291157      -0.000437   -0.055075 -0.188039       -0.069341    -0.100734   -0.300316     -0.063451      0.093137\n",
            "famrel             0.053940 -0.003914 -0.001370   -0.016808   0.039731 -0.044337  1.000000  0.150701  0.064568 -0.077594 -0.113397  0.094056 -0.044354  0.022168 -0.018281  -0.047926  0.058971   0.014258    -0.022776   0.025179    -0.061548    0.022736       0.056255     -0.022122    -0.013521    0.017534       0.051461     -0.069204    -0.012201     -0.019595          -0.000972        -0.010983        0.049548      -0.001345   -0.020436  0.000460        0.040687    -0.003581    0.024319      0.032768     -0.063816\n",
            "freetime           0.016434  0.030891 -0.012846   -0.017025  -0.143198  0.091987  0.150701  1.000000  0.285019  0.209001  0.147822  0.075733 -0.058078  0.012613 -0.013777   0.032988  0.238744   0.034878     0.017695   0.038717    -0.009094   -0.011640       0.021672      0.088508    -0.063738    0.040338      -0.051570      0.001675    -0.083249      0.039891          -0.050101        -0.028979        0.069438      -0.045465    0.010538 -0.064253        0.089728    -0.024696   -0.061244      0.051286     -0.011182\n",
            "goout              0.126964  0.064094  0.043105    0.028540  -0.063904  0.124561  0.064568  0.285019  1.000000  0.266994  0.420386 -0.009577  0.044302 -0.149104 -0.162250  -0.007152  0.075897   0.068835     0.023064   0.003459     0.059252   -0.001660       0.014455     -0.021319    -0.021394    0.047488      -0.010554     -0.018830    -0.004410     -0.015182          -0.038297         0.080236       -0.012380      -0.037698   -0.015631  0.010493        0.046088     0.004612   -0.039700      0.074370      0.007870\n",
            "Dalc               0.131125  0.019834  0.002386    0.138325  -0.196019  0.136047 -0.077594  0.209001  0.266994  1.000000  0.647544  0.077180  0.111908 -0.094159 -0.064120   0.114209  0.268171  -0.093494     0.101521  -0.030590    -0.074620    0.018872       0.028880      0.041015    -0.036273   -0.076526       0.111475      0.000552     0.022726      0.155076          -0.119213        -0.051224        0.037626      -0.021485   -0.031575  0.062465       -0.066508    -0.084849   -0.069828      0.036210      0.015121\n",
            "Walc               0.117276 -0.047123 -0.012631    0.134116  -0.253785  0.141962 -0.113397  0.147822  0.420386  0.647544  1.000000  0.092476  0.136291 -0.126179 -0.084927   0.065087  0.274194  -0.101126     0.103425   0.006045     0.021763   -0.024857      -0.004426      0.006194    -0.068333    0.050724       0.090568     -0.093898     0.005574      0.092452          -0.082733         0.006471       -0.045563      -0.087152   -0.086688  0.060454       -0.037477    -0.099534   -0.100340      0.011687     -0.010141\n",
            "health            -0.062187 -0.046878  0.014742    0.007501  -0.075616  0.065827  0.094056  0.075733 -0.009577  0.077180  0.092476  1.000000 -0.029937 -0.073172 -0.097720  -0.042651  0.143588  -0.040355    -0.028992   0.022307     0.046475   -0.034914       0.082612     -0.005960     0.078859   -0.001141      -0.038703      0.020423    -0.001766      0.006592          -0.145337        -0.025096       -0.045049      -0.034124    0.029297 -0.078132        0.023923    -0.018475   -0.015895     -0.080189      0.026342\n",
            "absences           0.175230  0.100285  0.024473   -0.012944  -0.062700  0.063726 -0.044354 -0.058078  0.044302  0.111908  0.136291 -0.029937  1.000000 -0.031003 -0.031777  -0.088480 -0.066962  -0.027874     0.035783  -0.134937    -0.044156    0.043005       0.023076      0.001688    -0.004191    0.016664       0.012907     -0.024964     0.111061     -0.003873           0.070671         0.023638        0.140826       0.022526    0.024353  0.007435       -0.013610     0.019155   -0.056085      0.101701      0.153384\n",
            "G1                -0.064081  0.205341  0.190270   -0.093040   0.160612 -0.354718  0.022168  0.012613 -0.149104 -0.094159 -0.126179 -0.073172 -0.031003  1.000000  0.852118  -0.025731  0.091839   0.069704     0.071445  -0.016868     0.120074   -0.164421       0.085906      0.078294     0.035303   -0.113924      -0.015093      0.168782    -0.017198     -0.007215           0.099522        -0.011767       -0.031022      -0.212607   -0.084569  0.039079        0.057010     0.069263    0.178264      0.071619     -0.037188\n",
            "G2                -0.143474  0.215527  0.164893   -0.153198   0.135880 -0.355896 -0.018281 -0.013777 -0.162250 -0.064120 -0.084927 -0.097720 -0.031777  0.852118  1.000000  -0.050086  0.091099   0.126037     0.081223  -0.041382     0.133893   -0.106426       0.078995      0.056343     0.039259   -0.089285       0.028117      0.096364     0.004798      0.042846           0.087008        -0.014442       -0.073712      -0.117385   -0.059166  0.105198        0.050552     0.068146    0.179129      0.119439     -0.111774\n",
            "school_MS          0.377610 -0.133333 -0.079807    0.242308  -0.090681  0.059804 -0.047926  0.032988 -0.007152  0.114209  0.065087 -0.042651 -0.088480 -0.025731 -0.050086   1.000000 -0.012286  -0.279797     0.064866   0.045923    -0.055139    0.042497      -0.053837     -0.016823    -0.079329   -0.067746       0.106639     -0.041674    -0.029905      0.131836          -0.129128        -0.064783        0.065759      -0.139789   -0.164967 -0.017083       -0.116946    -0.089277   -0.024150     -0.133578      0.060700\n",
            "sex_M             -0.028606  0.078228  0.034878    0.059722  -0.306268  0.044436  0.058971  0.238744  0.075897  0.268171  0.274194  0.143588 -0.066962  0.091839  0.091099  -0.012286  1.000000  -0.028504     0.089862   0.023443    -0.019817    0.002626       0.002748      0.165344    -0.061306    0.033307       0.005083      0.044147     0.049883      0.016859          -0.111434        -0.013642       -0.058525      -0.138271   -0.151623 -0.129126        0.099833    -0.008203   -0.151056      0.044113     -0.102023\n",
            "address_U         -0.146722  0.138804  0.072178   -0.328096  -0.020912 -0.078578  0.014258  0.034878  0.068835 -0.093494 -0.101126 -0.040355 -0.027874  0.069704  0.126037  -0.279797 -0.028504   1.000000     0.072472  -0.042572     0.099233   -0.032856       0.068549      0.033030     0.058641   -0.056929       0.023404      0.010748     0.153580     -0.041850          -0.077227        -0.081376        0.025177       0.024712    0.023903  0.052800       -0.051360     0.059589    0.042854      0.216842      0.005257\n",
            "famsize_LE3        0.037847 -0.043068 -0.058879    0.063493  -0.073595 -0.015769 -0.022776  0.017695  0.023064  0.101521  0.103425 -0.028992  0.035783  0.071445  0.081223   0.064866  0.089862   0.072472     1.000000  -0.149612     0.003732   -0.066398       0.041657      0.067259     0.021568    0.015408      -0.012871     -0.072183    -0.005728     -0.026982          -0.003842         0.026727       -0.025298      -0.028642   -0.112893 -0.013882       -0.000113     0.102088   -0.005806      0.000720      0.034395\n",
            "Pstatus_T          0.029598 -0.123565 -0.088730    0.028265   0.024294 -0.003339  0.025179  0.038717  0.003459 -0.030590  0.006045  0.022307 -0.134937 -0.016868 -0.041382   0.045923  0.023443  -0.042572    -0.149612   1.000000     0.015659   -0.023641      -0.062556      0.023927    -0.045041   -0.007940       0.083496     -0.063327    -0.031310      0.021247          -0.001903        -0.065813       -0.051061      -0.042238    0.019067  0.046435        0.097352    -0.090615   -0.040735      0.070074     -0.040451\n",
            "Mjob_health       -0.082660  0.251973  0.118501   -0.106708  -0.012977 -0.040859 -0.061548 -0.009094  0.059252 -0.074620  0.021763  0.046475 -0.044156  0.120074  0.133893  -0.055139 -0.019817   0.099233     0.003732   0.015659     1.000000   -0.228654      -0.182269     -0.127317     0.192628   -0.030450       0.008947     -0.051779    -0.007719      0.090990           0.080952        -0.048816       -0.024957      -0.064330    0.095788  0.080076        0.012616     0.066446    0.070874      0.089064      0.031343\n",
            "Mjob_other         0.032488 -0.235176 -0.197077    0.036544  -0.006288  0.006268  0.022736 -0.011640 -0.001660  0.018872 -0.024857 -0.034914  0.043005 -0.164421 -0.106426   0.042497  0.002626  -0.032856    -0.066398  -0.023641    -0.228654    1.000000      -0.442507     -0.309095    -0.112126    0.281851      -0.183657     -0.088168     0.095650     -0.089058           0.030130        -0.096649        0.030545       0.044043   -0.123493 -0.112524       -0.092480    -0.092740    0.003356     -0.048733      0.043476\n",
            "Mjob_services     -0.007732  0.051764  0.028032   -0.050943  -0.004476  0.105430  0.056255  0.021672  0.014455  0.028880 -0.004426  0.082612  0.023076  0.085906  0.078995  -0.053837  0.002748   0.068549     0.041657  -0.062556    -0.182269   -0.442507       1.000000     -0.246392    -0.019179   -0.169024       0.180310      0.009683    -0.044154      0.032310           0.034201        -0.014818        0.013857       0.046450    0.046119  0.009288        0.041377     0.030299    0.005660      0.080531     -0.041810\n",
            "Mjob_teacher      -0.058256  0.454911  0.294526   -0.051235  -0.009009 -0.157771 -0.022122  0.088508 -0.021319  0.041015  0.006194 -0.005960  0.001688  0.078294  0.056343  -0.016823  0.165344   0.033030     0.067259   0.023927    -0.127317   -0.309095      -0.246392      1.000000     0.046540   -0.156167       0.042985      0.212309    -0.000081     -0.031962          -0.039145         0.153480       -0.070746      -0.095745    0.065572  0.135270        0.107108     0.104411    0.063183      0.128292     -0.051288\n",
            "Fjob_health       -0.109842  0.083398  0.163216   -0.088277   0.121107 -0.016593 -0.013521 -0.063738 -0.021394 -0.036273 -0.068333  0.078859 -0.004191  0.035303  0.039259  -0.079329 -0.061306   0.058641     0.021568  -0.045041     0.192628   -0.112126      -0.019179      0.046540     1.000000   -0.241260      -0.136605     -0.061507    -0.053422      0.057341           0.115828        -0.064122       -0.020387       0.096873    0.098983  0.067047       -0.003873     0.050847    0.050462     -0.032293     -0.000391\n",
            "Fjob_other         0.007682 -0.109887 -0.253605    0.093214  -0.040619 -0.037792  0.017534  0.040338  0.047488 -0.076526  0.050724 -0.001141  0.016664 -0.113924 -0.089285  -0.067746  0.033307  -0.056929     0.015408  -0.007940    -0.030450    0.281851      -0.169024     -0.156167    -0.241260    1.000000      -0.690275     -0.310798     0.024121     -0.119815           0.026680         0.154429        0.026485      -0.045790   -0.020334  0.015977       -0.055193    -0.081931    0.046121     -0.064674     -0.059505\n",
            "Fjob_services      0.042963 -0.031834  0.021306   -0.030232   0.007162  0.082706  0.051461 -0.051570 -0.010554  0.111475  0.090568 -0.038703  0.012907 -0.015093  0.028117   0.106639  0.005083   0.023404    -0.012871   0.083496     0.008947   -0.183657       0.180310      0.042985    -0.136605   -0.690275       1.000000     -0.175979    -0.007943      0.075999          -0.044703        -0.130637        0.020799      -0.039163   -0.069429  0.001545        0.073416    -0.003320   -0.086822      0.098841      0.022762\n",
            "Fjob_teacher      -0.069993  0.259836  0.347203    0.014004  -0.058225 -0.074377 -0.069204  0.001675 -0.018830  0.000552 -0.093898  0.020423 -0.024964  0.168782  0.096364  -0.041674  0.044147   0.010748    -0.072183  -0.063327    -0.051779   -0.088168       0.009683      0.212309    -0.061507   -0.310798      -0.175979      1.000000    -0.021770      0.079490          -0.037547        -0.000904       -0.083576       0.065294    0.064417 -0.064065       -0.014697     0.046804    0.020735     -0.030037      0.026933\n",
            "reason_home        0.018283 -0.008708 -0.009618   -0.080027  -0.039620  0.042511 -0.012201 -0.083249 -0.004410  0.022726  0.005574 -0.001766  0.111061 -0.017198  0.004798  -0.029905  0.049883   0.153580    -0.005728  -0.031310    -0.007719    0.095650      -0.044154     -0.000081    -0.053422    0.024121      -0.007943     -0.021770     1.000000     -0.195494          -0.371472        -0.065389        0.086550      -0.001240    0.002561  0.080175       -0.050595     0.018964    0.065072      0.048775      0.030914\n",
            "reason_other       0.027172  0.008229 -0.014364   -0.001662  -0.107842 -0.012203 -0.019595  0.039891 -0.015182  0.155076  0.092452  0.006592 -0.003873 -0.007215  0.042846   0.131836  0.016859  -0.041850    -0.026982   0.021247     0.090990   -0.089058       0.032310     -0.031962     0.057341   -0.119815       0.075999      0.079490    -0.195494      1.000000          -0.190546         0.021304       -0.029545      -0.017001   -0.073234  0.061857       -0.023208    -0.013458   -0.127472     -0.023221      0.074026\n",
            "reason_reputation -0.031932  0.106487  0.048724   -0.033322   0.193342 -0.070127 -0.000972 -0.050101 -0.038297 -0.119213 -0.082733 -0.145337  0.070671  0.099522  0.087008  -0.129128 -0.111434  -0.077227    -0.003842  -0.001903     0.080952    0.030130       0.034201     -0.039145     0.115828    0.026680      -0.044703     -0.037547    -0.371472     -0.190546           1.000000        -0.019469        0.010368       0.024661    0.113767  0.067699        0.132624     0.050127    0.086686      0.039084     -0.025374\n",
            "guardian_mother   -0.133558  0.112375 -0.047265   -0.057669  -0.024031 -0.134498 -0.010983 -0.028979  0.080236 -0.051224  0.006471 -0.025096  0.023638 -0.011767 -0.014442  -0.064783 -0.013642  -0.081376     0.026727  -0.065813    -0.048816   -0.096649      -0.014818      0.153480    -0.064122    0.154429      -0.130637     -0.000904    -0.065389      0.021304          -0.019469         1.000000       -0.444143      -0.004054   -0.014124  0.064926        0.022809     0.108332   -0.004429     -0.020340     -0.037525\n",
            "guardian_other     0.398396 -0.118635 -0.091250    0.048758   0.031724  0.291157  0.049548  0.069438 -0.012380  0.037626 -0.045563 -0.045049  0.140826 -0.031022 -0.073712   0.065759 -0.058525   0.025177    -0.025298  -0.051061    -0.024957    0.030545       0.013857     -0.070746    -0.020387    0.026485       0.020799     -0.083576     0.086550     -0.029545           0.010368        -0.444143        1.000000      -0.058982    0.007522 -0.012352       -0.042382    -0.170929   -0.016071     -0.041116      0.124045\n",
            "schoolsup_yes     -0.251811 -0.036029  0.037530   -0.009246   0.037763 -0.000437 -0.001345 -0.045465 -0.037698 -0.021485 -0.087152 -0.034124  0.022526 -0.212607 -0.117385  -0.139789 -0.138271   0.024712    -0.028642  -0.042238    -0.064330    0.044043       0.046450     -0.095745     0.096873   -0.045790      -0.039163      0.065294    -0.001240     -0.017001           0.024661        -0.004054       -0.058982       1.000000    0.104681 -0.020753        0.046032     0.045967    0.054486     -0.009683     -0.080716\n",
            "famsup_yes        -0.140609  0.183727  0.185496   -0.003286   0.145228 -0.055075 -0.020436  0.010538 -0.015631 -0.031575 -0.086688  0.029297  0.024353 -0.084569 -0.059166  -0.164967 -0.151623   0.023903    -0.112893   0.019067     0.095788   -0.123493       0.046119      0.065572     0.098983   -0.020334      -0.069429      0.064417     0.002561     -0.073234           0.113767        -0.014124        0.007522       0.104681    1.000000  0.293184       -0.001500     0.059536    0.100815      0.103581      0.012440\n",
            "paid_yes          -0.035933  0.159700  0.086981   -0.066420   0.167220 -0.188039  0.000460 -0.064253  0.010493  0.062465  0.060454 -0.078132  0.007435  0.039079  0.105198  -0.017083 -0.129126   0.052800    -0.013882   0.046435     0.080076   -0.112524       0.009288      0.135270     0.067047    0.015977       0.001545     -0.064065     0.080175      0.061857           0.067699         0.064926       -0.012352      -0.020753    0.293184  1.000000       -0.021382     0.102143    0.189214      0.153132      0.005536\n",
            "activities_yes    -0.103063  0.108277  0.112643   -0.007766   0.089877 -0.069341  0.040687  0.089728  0.046088 -0.066508 -0.037477  0.023923 -0.013610  0.057010  0.050552  -0.116946  0.099833  -0.051360    -0.000113   0.097352     0.012616   -0.092480       0.041377      0.107108    -0.003873   -0.055193       0.073416     -0.014697    -0.050595     -0.023208           0.132624         0.022809       -0.042382       0.046032   -0.001500 -0.021382        1.000000     0.002731    0.096484      0.048663      0.019651\n",
            "nursery_yes       -0.086632  0.193263  0.157177   -0.033338   0.081325 -0.100734 -0.003581 -0.024696  0.004612 -0.084849 -0.099534 -0.018475  0.019155  0.069263  0.068146  -0.089277 -0.008203   0.059589     0.102088  -0.090615     0.066446   -0.092740       0.030299      0.104411     0.050847   -0.081931      -0.003320      0.046804     0.018964     -0.013458           0.050127         0.108332       -0.170929       0.045967    0.059536  0.102143        0.002731     1.000000    0.054303      0.007830      0.027495\n",
            "higher_yes        -0.209081  0.168845  0.174566   -0.083508   0.175081 -0.300316  0.024319 -0.061244 -0.039700 -0.069828 -0.100340 -0.015895 -0.056085  0.178264  0.179129  -0.024150 -0.151056   0.042854    -0.005806  -0.040735     0.070874    0.003356       0.005660      0.063183     0.050462    0.046121      -0.086822      0.020735     0.065072     -0.127472           0.086686        -0.004429       -0.016071       0.054486    0.100815  0.189214        0.096484     0.054303    1.000000      0.020374     -0.105664\n",
            "internet_yes      -0.112094  0.201463  0.127507   -0.111302   0.059422 -0.063451  0.032768  0.051286  0.074370  0.036210  0.011687 -0.080189  0.101701  0.071619  0.119439  -0.133578  0.044113   0.216842     0.000720   0.070074     0.089064   -0.048733       0.080531      0.128292    -0.032293   -0.064674       0.098841     -0.030037     0.048775     -0.023221           0.039084        -0.020340       -0.041116      -0.009683    0.103581  0.153132        0.048663     0.007830    0.020374      1.000000      0.087122\n",
            "romantic_yes       0.164669  0.039681  0.015602    0.021962   0.053285  0.093137 -0.063816 -0.011182  0.007870  0.015121 -0.010141  0.026342  0.153384 -0.037188 -0.111774   0.060700 -0.102023   0.005257     0.034395  -0.040451     0.031343    0.043476      -0.041810     -0.051288    -0.000391   -0.059505       0.022762      0.026933     0.030914      0.074026          -0.025374        -0.037525        0.124045      -0.080716    0.012440  0.005536        0.019651     0.027495   -0.105664      0.087122      1.000000\n",
            "\n",
            "\n",
            "--- üî¥ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Threshold: > 0.45) üî¥ ---\n",
            "\n",
            "--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 1 ---\n",
            "1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 0.8521):\n",
            "          G1        G2\n",
            "G1  0.000000  0.852118\n",
            "G2  0.852118  0.000000\n",
            "\n",
            "3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\n",
            "   * **G1**: S.D. = **0.20745**\n",
            "   * **G2**: S.D. = **0.19797**\n",
            "4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\n",
            "   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'G1' (S.D. **0.20745**)\n",
            "   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'G2' (S.D. 0.19797)\n",
            "\n",
            "--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 2 ---\n",
            "1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 0.6903):\n",
            "               Fjob_other  Fjob_services\n",
            "Fjob_other       0.000000       0.690275\n",
            "Fjob_services    0.690275       0.000000\n",
            "\n",
            "3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\n",
            "   * **Fjob_other**: S.D. = **0.49819**\n",
            "   * **Fjob_services**: S.D. = **0.45006**\n",
            "4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\n",
            "   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Fjob_other' (S.D. **0.49819**)\n",
            "   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Fjob_services' (S.D. 0.45006)\n",
            "\n",
            "--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 3 ---\n",
            "1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 0.6475):\n",
            "          Dalc      Walc\n",
            "Dalc  0.000000  0.647544\n",
            "Walc  0.647544  0.000000\n",
            "\n",
            "3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\n",
            "   * **Dalc**: S.D. = **0.22269**\n",
            "   * **Walc**: S.D. = **0.32197**\n",
            "4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\n",
            "   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Walc' (S.D. **0.32197**)\n",
            "   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Dalc' (S.D. 0.22269)\n",
            "\n",
            "--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 4 ---\n",
            "1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 0.6235):\n",
            "          Medu      Fedu\n",
            "Medu  0.000000  0.623455\n",
            "Fedu  0.623455  0.000000\n",
            "\n",
            "3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\n",
            "   * **Medu**: S.D. = **0.27368**\n",
            "   * **Fedu**: S.D. = **0.27205**\n",
            "4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\n",
            "   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Medu' (S.D. **0.27368**)\n",
            "   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Fedu' (S.D. 0.27205)\n",
            "\n",
            "--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 5 ---\n",
            "1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 0.4549):\n",
            "                  Medu  Mjob_teacher\n",
            "Medu          0.000000      0.454911\n",
            "Mjob_teacher  0.454911      0.000000\n",
            "\n",
            "3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\n",
            "   * **Medu**: S.D. = **0.27368**\n",
            "   * **Mjob_teacher**: S.D. = **0.35439**\n",
            "4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\n",
            "   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Mjob_teacher' (S.D. **0.35439**)\n",
            "   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'Medu' (S.D. 0.27368)\n",
            "\n",
            "‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå ( 0.45 )\n",
            "\n",
            "--- Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---\n",
            "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: (36, 36)\n",
            "                        age  traveltime  studytime  failures    famrel  freetime     goout      Walc    health  absences        G1  school_MS     sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_teacher  reason_home  reason_other  reason_reputation  guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  activities_yes  nursery_yes  higher_yes  internet_yes  romantic_yes\n",
            "age                1.000000    0.070641  -0.004140  0.243665  0.053940  0.016434  0.126964  0.117276 -0.062187  0.175230 -0.064081   0.377610 -0.028606  -0.146722     0.037847   0.029598    -0.082660    0.032488      -0.007732     -0.058256    -0.109842    0.007682     -0.069993     0.018283      0.027172          -0.031932        -0.133558        0.398396      -0.251811   -0.140609 -0.035933       -0.103063    -0.086632   -0.209081     -0.112094      0.164669\n",
            "traveltime         0.070641    1.000000  -0.100909  0.092239 -0.016808 -0.017025  0.028540  0.134116  0.007501 -0.012944 -0.093040   0.242308  0.059722  -0.328096     0.063493   0.028265    -0.106708    0.036544      -0.050943     -0.051235    -0.088277    0.093214      0.014004    -0.080027     -0.001662          -0.033322        -0.057669        0.048758      -0.009246   -0.003286 -0.066420       -0.007766    -0.033338   -0.083508     -0.111302      0.021962\n",
            "studytime         -0.004140   -0.100909   1.000000 -0.173563  0.039731 -0.143198 -0.063904 -0.253785 -0.075616 -0.062700  0.160612  -0.090681 -0.306268  -0.020912    -0.073595   0.024294    -0.012977   -0.006288      -0.004476     -0.009009     0.121107   -0.040619     -0.058225    -0.039620     -0.107842           0.193342        -0.024031        0.031724       0.037763    0.145228  0.167220        0.089877     0.081325    0.175081      0.059422      0.053285\n",
            "failures           0.243665    0.092239  -0.173563  1.000000 -0.044337  0.091987  0.124561  0.141962  0.065827  0.063726 -0.354718   0.059804  0.044436  -0.078578    -0.015769  -0.003339    -0.040859    0.006268       0.105430     -0.157771    -0.016593   -0.037792     -0.074377     0.042511     -0.012203          -0.070127        -0.134498        0.291157      -0.000437   -0.055075 -0.188039       -0.069341    -0.100734   -0.300316     -0.063451      0.093137\n",
            "famrel             0.053940   -0.016808   0.039731 -0.044337  1.000000  0.150701  0.064568 -0.113397  0.094056 -0.044354  0.022168  -0.047926  0.058971   0.014258    -0.022776   0.025179    -0.061548    0.022736       0.056255     -0.022122    -0.013521    0.017534     -0.069204    -0.012201     -0.019595          -0.000972        -0.010983        0.049548      -0.001345   -0.020436  0.000460        0.040687    -0.003581    0.024319      0.032768     -0.063816\n",
            "freetime           0.016434   -0.017025  -0.143198  0.091987  0.150701  1.000000  0.285019  0.147822  0.075733 -0.058078  0.012613   0.032988  0.238744   0.034878     0.017695   0.038717    -0.009094   -0.011640       0.021672      0.088508    -0.063738    0.040338      0.001675    -0.083249      0.039891          -0.050101        -0.028979        0.069438      -0.045465    0.010538 -0.064253        0.089728    -0.024696   -0.061244      0.051286     -0.011182\n",
            "goout              0.126964    0.028540  -0.063904  0.124561  0.064568  0.285019  1.000000  0.420386 -0.009577  0.044302 -0.149104  -0.007152  0.075897   0.068835     0.023064   0.003459     0.059252   -0.001660       0.014455     -0.021319    -0.021394    0.047488     -0.018830    -0.004410     -0.015182          -0.038297         0.080236       -0.012380      -0.037698   -0.015631  0.010493        0.046088     0.004612   -0.039700      0.074370      0.007870\n",
            "Walc               0.117276    0.134116  -0.253785  0.141962 -0.113397  0.147822  0.420386  1.000000  0.092476  0.136291 -0.126179   0.065087  0.274194  -0.101126     0.103425   0.006045     0.021763   -0.024857      -0.004426      0.006194    -0.068333    0.050724     -0.093898     0.005574      0.092452          -0.082733         0.006471       -0.045563      -0.087152   -0.086688  0.060454       -0.037477    -0.099534   -0.100340      0.011687     -0.010141\n",
            "health            -0.062187    0.007501  -0.075616  0.065827  0.094056  0.075733 -0.009577  0.092476  1.000000 -0.029937 -0.073172  -0.042651  0.143588  -0.040355    -0.028992   0.022307     0.046475   -0.034914       0.082612     -0.005960     0.078859   -0.001141      0.020423    -0.001766      0.006592          -0.145337        -0.025096       -0.045049      -0.034124    0.029297 -0.078132        0.023923    -0.018475   -0.015895     -0.080189      0.026342\n",
            "absences           0.175230   -0.012944  -0.062700  0.063726 -0.044354 -0.058078  0.044302  0.136291 -0.029937  1.000000 -0.031003  -0.088480 -0.066962  -0.027874     0.035783  -0.134937    -0.044156    0.043005       0.023076      0.001688    -0.004191    0.016664     -0.024964     0.111061     -0.003873           0.070671         0.023638        0.140826       0.022526    0.024353  0.007435       -0.013610     0.019155   -0.056085      0.101701      0.153384\n",
            "G1                -0.064081   -0.093040   0.160612 -0.354718  0.022168  0.012613 -0.149104 -0.126179 -0.073172 -0.031003  1.000000  -0.025731  0.091839   0.069704     0.071445  -0.016868     0.120074   -0.164421       0.085906      0.078294     0.035303   -0.113924      0.168782    -0.017198     -0.007215           0.099522        -0.011767       -0.031022      -0.212607   -0.084569  0.039079        0.057010     0.069263    0.178264      0.071619     -0.037188\n",
            "school_MS          0.377610    0.242308  -0.090681  0.059804 -0.047926  0.032988 -0.007152  0.065087 -0.042651 -0.088480 -0.025731   1.000000 -0.012286  -0.279797     0.064866   0.045923    -0.055139    0.042497      -0.053837     -0.016823    -0.079329   -0.067746     -0.041674    -0.029905      0.131836          -0.129128        -0.064783        0.065759      -0.139789   -0.164967 -0.017083       -0.116946    -0.089277   -0.024150     -0.133578      0.060700\n",
            "sex_M             -0.028606    0.059722  -0.306268  0.044436  0.058971  0.238744  0.075897  0.274194  0.143588 -0.066962  0.091839  -0.012286  1.000000  -0.028504     0.089862   0.023443    -0.019817    0.002626       0.002748      0.165344    -0.061306    0.033307      0.044147     0.049883      0.016859          -0.111434        -0.013642       -0.058525      -0.138271   -0.151623 -0.129126        0.099833    -0.008203   -0.151056      0.044113     -0.102023\n",
            "address_U         -0.146722   -0.328096  -0.020912 -0.078578  0.014258  0.034878  0.068835 -0.101126 -0.040355 -0.027874  0.069704  -0.279797 -0.028504   1.000000     0.072472  -0.042572     0.099233   -0.032856       0.068549      0.033030     0.058641   -0.056929      0.010748     0.153580     -0.041850          -0.077227        -0.081376        0.025177       0.024712    0.023903  0.052800       -0.051360     0.059589    0.042854      0.216842      0.005257\n",
            "famsize_LE3        0.037847    0.063493  -0.073595 -0.015769 -0.022776  0.017695  0.023064  0.103425 -0.028992  0.035783  0.071445   0.064866  0.089862   0.072472     1.000000  -0.149612     0.003732   -0.066398       0.041657      0.067259     0.021568    0.015408     -0.072183    -0.005728     -0.026982          -0.003842         0.026727       -0.025298      -0.028642   -0.112893 -0.013882       -0.000113     0.102088   -0.005806      0.000720      0.034395\n",
            "Pstatus_T          0.029598    0.028265   0.024294 -0.003339  0.025179  0.038717  0.003459  0.006045  0.022307 -0.134937 -0.016868   0.045923  0.023443  -0.042572    -0.149612   1.000000     0.015659   -0.023641      -0.062556      0.023927    -0.045041   -0.007940     -0.063327    -0.031310      0.021247          -0.001903        -0.065813       -0.051061      -0.042238    0.019067  0.046435        0.097352    -0.090615   -0.040735      0.070074     -0.040451\n",
            "Mjob_health       -0.082660   -0.106708  -0.012977 -0.040859 -0.061548 -0.009094  0.059252  0.021763  0.046475 -0.044156  0.120074  -0.055139 -0.019817   0.099233     0.003732   0.015659     1.000000   -0.228654      -0.182269     -0.127317     0.192628   -0.030450     -0.051779    -0.007719      0.090990           0.080952        -0.048816       -0.024957      -0.064330    0.095788  0.080076        0.012616     0.066446    0.070874      0.089064      0.031343\n",
            "Mjob_other         0.032488    0.036544  -0.006288  0.006268  0.022736 -0.011640 -0.001660 -0.024857 -0.034914  0.043005 -0.164421   0.042497  0.002626  -0.032856    -0.066398  -0.023641    -0.228654    1.000000      -0.442507     -0.309095    -0.112126    0.281851     -0.088168     0.095650     -0.089058           0.030130        -0.096649        0.030545       0.044043   -0.123493 -0.112524       -0.092480    -0.092740    0.003356     -0.048733      0.043476\n",
            "Mjob_services     -0.007732   -0.050943  -0.004476  0.105430  0.056255  0.021672  0.014455 -0.004426  0.082612  0.023076  0.085906  -0.053837  0.002748   0.068549     0.041657  -0.062556    -0.182269   -0.442507       1.000000     -0.246392    -0.019179   -0.169024      0.009683    -0.044154      0.032310           0.034201        -0.014818        0.013857       0.046450    0.046119  0.009288        0.041377     0.030299    0.005660      0.080531     -0.041810\n",
            "Mjob_teacher      -0.058256   -0.051235  -0.009009 -0.157771 -0.022122  0.088508 -0.021319  0.006194 -0.005960  0.001688  0.078294  -0.016823  0.165344   0.033030     0.067259   0.023927    -0.127317   -0.309095      -0.246392      1.000000     0.046540   -0.156167      0.212309    -0.000081     -0.031962          -0.039145         0.153480       -0.070746      -0.095745    0.065572  0.135270        0.107108     0.104411    0.063183      0.128292     -0.051288\n",
            "Fjob_health       -0.109842   -0.088277   0.121107 -0.016593 -0.013521 -0.063738 -0.021394 -0.068333  0.078859 -0.004191  0.035303  -0.079329 -0.061306   0.058641     0.021568  -0.045041     0.192628   -0.112126      -0.019179      0.046540     1.000000   -0.241260     -0.061507    -0.053422      0.057341           0.115828        -0.064122       -0.020387       0.096873    0.098983  0.067047       -0.003873     0.050847    0.050462     -0.032293     -0.000391\n",
            "Fjob_other         0.007682    0.093214  -0.040619 -0.037792  0.017534  0.040338  0.047488  0.050724 -0.001141  0.016664 -0.113924  -0.067746  0.033307  -0.056929     0.015408  -0.007940    -0.030450    0.281851      -0.169024     -0.156167    -0.241260    1.000000     -0.310798     0.024121     -0.119815           0.026680         0.154429        0.026485      -0.045790   -0.020334  0.015977       -0.055193    -0.081931    0.046121     -0.064674     -0.059505\n",
            "Fjob_teacher      -0.069993    0.014004  -0.058225 -0.074377 -0.069204  0.001675 -0.018830 -0.093898  0.020423 -0.024964  0.168782  -0.041674  0.044147   0.010748    -0.072183  -0.063327    -0.051779   -0.088168       0.009683      0.212309    -0.061507   -0.310798      1.000000    -0.021770      0.079490          -0.037547        -0.000904       -0.083576       0.065294    0.064417 -0.064065       -0.014697     0.046804    0.020735     -0.030037      0.026933\n",
            "reason_home        0.018283   -0.080027  -0.039620  0.042511 -0.012201 -0.083249 -0.004410  0.005574 -0.001766  0.111061 -0.017198  -0.029905  0.049883   0.153580    -0.005728  -0.031310    -0.007719    0.095650      -0.044154     -0.000081    -0.053422    0.024121     -0.021770     1.000000     -0.195494          -0.371472        -0.065389        0.086550      -0.001240    0.002561  0.080175       -0.050595     0.018964    0.065072      0.048775      0.030914\n",
            "reason_other       0.027172   -0.001662  -0.107842 -0.012203 -0.019595  0.039891 -0.015182  0.092452  0.006592 -0.003873 -0.007215   0.131836  0.016859  -0.041850    -0.026982   0.021247     0.090990   -0.089058       0.032310     -0.031962     0.057341   -0.119815      0.079490    -0.195494      1.000000          -0.190546         0.021304       -0.029545      -0.017001   -0.073234  0.061857       -0.023208    -0.013458   -0.127472     -0.023221      0.074026\n",
            "reason_reputation -0.031932   -0.033322   0.193342 -0.070127 -0.000972 -0.050101 -0.038297 -0.082733 -0.145337  0.070671  0.099522  -0.129128 -0.111434  -0.077227    -0.003842  -0.001903     0.080952    0.030130       0.034201     -0.039145     0.115828    0.026680     -0.037547    -0.371472     -0.190546           1.000000        -0.019469        0.010368       0.024661    0.113767  0.067699        0.132624     0.050127    0.086686      0.039084     -0.025374\n",
            "guardian_mother   -0.133558   -0.057669  -0.024031 -0.134498 -0.010983 -0.028979  0.080236  0.006471 -0.025096  0.023638 -0.011767  -0.064783 -0.013642  -0.081376     0.026727  -0.065813    -0.048816   -0.096649      -0.014818      0.153480    -0.064122    0.154429     -0.000904    -0.065389      0.021304          -0.019469         1.000000       -0.444143      -0.004054   -0.014124  0.064926        0.022809     0.108332   -0.004429     -0.020340     -0.037525\n",
            "guardian_other     0.398396    0.048758   0.031724  0.291157  0.049548  0.069438 -0.012380 -0.045563 -0.045049  0.140826 -0.031022   0.065759 -0.058525   0.025177    -0.025298  -0.051061    -0.024957    0.030545       0.013857     -0.070746    -0.020387    0.026485     -0.083576     0.086550     -0.029545           0.010368        -0.444143        1.000000      -0.058982    0.007522 -0.012352       -0.042382    -0.170929   -0.016071     -0.041116      0.124045\n",
            "schoolsup_yes     -0.251811   -0.009246   0.037763 -0.000437 -0.001345 -0.045465 -0.037698 -0.087152 -0.034124  0.022526 -0.212607  -0.139789 -0.138271   0.024712    -0.028642  -0.042238    -0.064330    0.044043       0.046450     -0.095745     0.096873   -0.045790      0.065294    -0.001240     -0.017001           0.024661        -0.004054       -0.058982       1.000000    0.104681 -0.020753        0.046032     0.045967    0.054486     -0.009683     -0.080716\n",
            "famsup_yes        -0.140609   -0.003286   0.145228 -0.055075 -0.020436  0.010538 -0.015631 -0.086688  0.029297  0.024353 -0.084569  -0.164967 -0.151623   0.023903    -0.112893   0.019067     0.095788   -0.123493       0.046119      0.065572     0.098983   -0.020334      0.064417     0.002561     -0.073234           0.113767        -0.014124        0.007522       0.104681    1.000000  0.293184       -0.001500     0.059536    0.100815      0.103581      0.012440\n",
            "paid_yes          -0.035933   -0.066420   0.167220 -0.188039  0.000460 -0.064253  0.010493  0.060454 -0.078132  0.007435  0.039079  -0.017083 -0.129126   0.052800    -0.013882   0.046435     0.080076   -0.112524       0.009288      0.135270     0.067047    0.015977     -0.064065     0.080175      0.061857           0.067699         0.064926       -0.012352      -0.020753    0.293184  1.000000       -0.021382     0.102143    0.189214      0.153132      0.005536\n",
            "activities_yes    -0.103063   -0.007766   0.089877 -0.069341  0.040687  0.089728  0.046088 -0.037477  0.023923 -0.013610  0.057010  -0.116946  0.099833  -0.051360    -0.000113   0.097352     0.012616   -0.092480       0.041377      0.107108    -0.003873   -0.055193     -0.014697    -0.050595     -0.023208           0.132624         0.022809       -0.042382       0.046032   -0.001500 -0.021382        1.000000     0.002731    0.096484      0.048663      0.019651\n",
            "nursery_yes       -0.086632   -0.033338   0.081325 -0.100734 -0.003581 -0.024696  0.004612 -0.099534 -0.018475  0.019155  0.069263  -0.089277 -0.008203   0.059589     0.102088  -0.090615     0.066446   -0.092740       0.030299      0.104411     0.050847   -0.081931      0.046804     0.018964     -0.013458           0.050127         0.108332       -0.170929       0.045967    0.059536  0.102143        0.002731     1.000000    0.054303      0.007830      0.027495\n",
            "higher_yes        -0.209081   -0.083508   0.175081 -0.300316  0.024319 -0.061244 -0.039700 -0.100340 -0.015895 -0.056085  0.178264  -0.024150 -0.151056   0.042854    -0.005806  -0.040735     0.070874    0.003356       0.005660      0.063183     0.050462    0.046121      0.020735     0.065072     -0.127472           0.086686        -0.004429       -0.016071       0.054486    0.100815  0.189214        0.096484     0.054303    1.000000      0.020374     -0.105664\n",
            "internet_yes      -0.112094   -0.111302   0.059422 -0.063451  0.032768  0.051286  0.074370  0.011687 -0.080189  0.101701  0.071619  -0.133578  0.044113   0.216842     0.000720   0.070074     0.089064   -0.048733       0.080531      0.128292    -0.032293   -0.064674     -0.030037     0.048775     -0.023221           0.039084        -0.020340       -0.041116      -0.009683    0.103581  0.153132        0.048663     0.007830    0.020374      1.000000      0.087122\n",
            "romantic_yes       0.164669    0.021962   0.053285  0.093137 -0.063816 -0.011182  0.007870 -0.010141  0.026342  0.153384 -0.037188   0.060700 -0.102023   0.005257     0.034395  -0.040451     0.031343    0.043476      -0.041810     -0.051288    -0.000391   -0.059505      0.026933     0.030914      0.074026          -0.025374        -0.037525        0.124045      -0.080716    0.012440  0.005536        0.019651     0.027495   -0.105664      0.087122      1.000000\n",
            "--- üèÅ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ üèÅ ---\n",
            "\n",
            "--- üèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° S.D.) üèÜ ---\n",
            "‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 36 ‡∏ï‡∏±‡∏ß\n",
            "              Feature Standard Deviation (S.D.)\n",
            "0      activities_yes                  0.500555\n",
            "1               sex_M                  0.499926\n",
            "2            paid_yes                  0.498884\n",
            "3          Fjob_other                  0.498188\n",
            "4          famsup_yes                  0.487761\n",
            "5          Mjob_other                  0.479711\n",
            "6        romantic_yes                    0.4723\n",
            "7     guardian_mother                   0.46261\n",
            "8         famsize_LE3                   0.45369\n",
            "9         reason_home                  0.447558\n",
            "10  reason_reputation                  0.442331\n",
            "11      Mjob_services                  0.439606\n",
            "12          address_U                  0.416643\n",
            "13        nursery_yes                   0.40426\n",
            "14       internet_yes                  0.373528\n",
            "15       Mjob_teacher                  0.354391\n",
            "16             health                  0.347576\n",
            "17      schoolsup_yes                  0.335751\n",
            "18               Walc                  0.321974\n",
            "19          school_MS                  0.321177\n",
            "20          Pstatus_T                  0.305384\n",
            "21       reason_other                  0.288172\n",
            "22        Mjob_health                  0.280832\n",
            "23          studytime                  0.279747\n",
            "24              goout                   0.27832\n",
            "25     guardian_other                  0.273201\n",
            "26       Fjob_teacher                  0.261152\n",
            "27           freetime                  0.249716\n",
            "28           failures                  0.247884\n",
            "29         traveltime                  0.232502\n",
            "30             famrel                  0.224165\n",
            "31         higher_yes                  0.219525\n",
            "32        Fjob_health                  0.208814\n",
            "33                 G1                   0.20745\n",
            "34                age                  0.182292\n",
            "35           absences                  0.106708\n",
            "\n",
            "\n",
            "Accuracy (Features Selected by Correlation/SD): 0.8151260504201681\n",
            "\n",
            "\n",
            " Features \n",
            "Accuracy (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å Features): 0.8991596638655462\n",
            "\n",
            "\n",
            "Threshold = 0.03\n",
            "Accuracy: 0.8907563025210085\n",
            "\n",
            "\n",
            " Best Features (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° Variance)\n",
            "Accuracy (Top 3): 0.4789915966386555\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}