{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibv5Wxe6t6LY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import MinMaxScaler  <-- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏≠‡∏≤‡∏≠‡∏≠‡∏Å\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\")\n",
        "print(df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" MinMax (Manual Scaling) \")\n",
        "# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Min-Max Scaling ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á\n",
        "denominator = X_max - X_min\n",
        "# ‡πÉ‡∏ä‡πâ np.where ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà X_max == X_min (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå)\n",
        "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Iris ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏î‡∏µ\n",
        "X_scaled = np.where(denominator == 0, 0, (X - X_min) / denominator)\n",
        "# --- ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "print(df_scaled.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Variance \")\n",
        "variances = X_scaled.var(axis=0)\n",
        "for i in range(4):\n",
        "    print(feature_names[i], \":\", variances[i])\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Features \")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_1 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc_1)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Threshold = 0.03\")\n",
        "selector = VarianceThreshold(threshold=0.03)\n",
        "X_new_2 = selector.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new_2, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_2 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc_2)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Best 3 Features\")\n",
        "sorted_idx = np.argsort(variances)\n",
        "top3_idx = sorted_idx[-3:]\n",
        "X_best3 = X_scaled[:, top3_idx]\n",
        "X_train, X_test, y_train, y_test_3, = train_test_split(X_best3, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_3 = model.predict(X_test)\n",
        "acc_3 = accuracy_score(y_test_3, y_pred_3)\n",
        "print(\"Accuracy:\", acc_3)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Best 2 Features\")\n",
        "sorted_idx = np.argsort(variances)\n",
        "top2_idx = sorted_idx[-2:]\n",
        "X_best2 = X_scaled[:, top2_idx]\n",
        "X_train, X_test, y_train, y_test_2 = train_test_split(X_best2, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_2 = model.predict(X_test)\n",
        "acc_4 = accuracy_score(y_test_2, y_pred_2)\n",
        "print(\"Accuracy:\", acc_4)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"\\nBest 3 Features\")\n",
        "print(\"Accuracy:\", acc_3)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_3, y_pred_3))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_3, y_pred_3, target_names=iris.target_names))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n Best 2 Features \")\n",
        "print(\"Accuracy:\", acc_4)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_2, y_pred_2))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_2, y_pred_2, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"step1.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1NG1zPbJTgl8GwuIAMicEZm9Fb07nCx5H\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import MinMaxScaler  <-- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏≠‡∏≤‡∏≠‡∏≠‡∏Å\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\")\n",
        "print(df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" MinMax (Manual Scaling) \")\n",
        "# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Min-Max Scaling ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á\n",
        "denominator = X_max - X_min\n",
        "# ‡πÉ‡∏ä‡πâ np.where ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà X_max == X_min (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå)\n",
        "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Iris ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏î‡∏µ\n",
        "X_scaled = np.where(denominator == 0, 0, (X - X_min) / denominator)\n",
        "# --- ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Manual ---\n",
        "\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "print(df_scaled.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Variance \")\n",
        "variances = X_scaled.var(axis=0)\n",
        "for i in range(4):\n",
        "    print(feature_names[i], \":\", variances[i])\n",
        "print(\"\\n\")\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô **\n",
        "# *******************************************************************\n",
        "print(\"--- Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ---\")\n",
        "correlation_matrix = df_scaled.corr()\n",
        "print(correlation_matrix)\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "# *******************************************************************\n",
        "# ** ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ: ‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Correlation-Based Feature Selection) **\n",
        "# *******************************************************************\n",
        "\n",
        "def select_features_by_correlation(df, threshold=0.85):\n",
        "    \"\"\"\n",
        "    ‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
        "    ‡πÇ‡∏î‡∏¢‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Standard Deviation (S.D.) ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏ß‡πâ\n",
        "    \"\"\"\n",
        "    print(\"--- üî¥ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Threshold: > %.2f) üî¥ ---\" % threshold)\n",
        "    df_current = df.copy()\n",
        "\n",
        "    round_count = 1\n",
        "\n",
        "    while True:\n",
        "        # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "        corr_matrix = df_current.corr().abs()\n",
        "\n",
        "        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏ô‡∏ß‡∏ó‡πÅ‡∏¢‡∏á‡∏°‡∏∏‡∏°)\n",
        "        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏ô‡∏ß‡∏ó‡πÅ‡∏¢‡∏á‡∏°‡∏∏‡∏°‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ 1.0 ‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
        "        np.fill_diagonal(corr_matrix.values, 0)\n",
        "\n",
        "        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "        max_corr_value = corr_matrix.max().max()\n",
        "\n",
        "        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (Threshold)\n",
        "        if max_corr_value <= threshold:\n",
        "            print(\"\\n‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå (\", threshold, \")\")\n",
        "\n",
        "            # ** ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Correlation Matrix ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£) **\n",
        "            final_corr_matrix = df_current.corr()\n",
        "            print(\"\\n--- Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---\")\n",
        "            print(final_corr_matrix)\n",
        "\n",
        "            break\n",
        "\n",
        "        # ‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "        r, c = np.unravel_index(corr_matrix.values.argmax(), corr_matrix.shape)\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
        "        feature_a = corr_matrix.columns[r]\n",
        "        feature_b = corr_matrix.columns[c]\n",
        "\n",
        "        print(\"\\n--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà %d ---\" % round_count)\n",
        "\n",
        "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1)\n",
        "        print(\"1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:\")\n",
        "        print(corr_matrix)\n",
        "        print(\"\\n2. ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (%.4f): %s vs %s\" % (max_corr_value, feature_a, feature_b))\n",
        "\n",
        "        # 4. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (Standard Deviation)\n",
        "        std_a = df_current[feature_a].std()\n",
        "        std_b = df_current[feature_b].std()\n",
        "\n",
        "        # *‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3*\n",
        "        print(\"3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\")\n",
        "        print(\"   * **%s**: S.D. = **%.5f**\" % (feature_a, std_a))\n",
        "        print(\"   * **%s**: S.D. = **%.5f**\" % (feature_b, std_b))\n",
        "\n",
        "        # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à: ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ S.D. ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ S.D. ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á)\n",
        "        if std_a >= std_b:\n",
        "            feature_to_drop = feature_b\n",
        "            feature_to_keep = feature_a\n",
        "        else:\n",
        "            feature_to_drop = feature_a\n",
        "            feature_to_keep = feature_b\n",
        "\n",
        "        # ‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ S.D. ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á\n",
        "        df_current = df_current.drop(columns=[feature_to_drop])\n",
        "\n",
        "        # *‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4*\n",
        "        print(\"4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\")\n",
        "        print(\"   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: '%s' (S.D. **%.5f**)\" % (feature_to_keep, max(std_a, std_b)))\n",
        "        print(\"   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: '%s' (S.D. %.5f)\" % (feature_to_drop, min(std_a, std_b)))\n",
        "\n",
        "        round_count += 1\n",
        "\n",
        "    print(\"--- üèÅ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ üèÅ ---\")\n",
        "    print(\"‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠:\", list(df_current.columns))\n",
        "    return df_current\n",
        "\n",
        "# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞\n",
        "df_selected = select_features_by_correlation(df_scaled, threshold=0.85)\n",
        "X_selected = df_selected.values\n",
        "feature_names_selected = df_selected.columns.tolist()\n",
        "\n",
        "print(\"\\n--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ ---\")\n",
        "print(\"‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:\", feature_names_selected)\n",
        "\n",
        "# ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
        "X_train, X_test, y_train, y_test_sel = train_test_split(X_selected, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_sel = model.predict(X_test)\n",
        "acc_sel = accuracy_score(y_test_sel, y_pred_sel)\n",
        "print(\"Accuracy (Features Selected by Correlation/SD):\", acc_sel)\n",
        "print(\"\\n\")\n",
        "# *******************************************************************\n",
        "\n",
        "\n",
        "print(\" Features \")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_1 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc_1)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Threshold = 0.03\")\n",
        "selector = VarianceThreshold(threshold=0.03)\n",
        "X_new_2 = selector.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new_2, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc_2 = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc_2)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Best 3 Features\")\n",
        "sorted_idx = np.argsort(variances)\n",
        "top3_idx = sorted_idx[-3:]\n",
        "X_best3 = X_scaled[:, top3_idx]\n",
        "X_train, X_test, y_train, y_test_3, = train_test_split(X_best3, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_3 = model.predict(X_test)\n",
        "acc_3 = accuracy_score(y_test_3, y_pred_3)\n",
        "print(\"Accuracy:\", acc_3)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\" Best 2 Features\")\n",
        "sorted_idx = np.argsort(variances)\n",
        "top2_idx = sorted_idx[-2:]\n",
        "X_best2 = X_scaled[:, top2_idx]\n",
        "X_train, X_test, y_train, y_test_2 = train_test_split(X_best2, y, test_size=0.3,  stratify=y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_2 = model.predict(X_test)\n",
        "acc_4 = accuracy_score(y_test_2, y_pred_2)\n",
        "print(\"Accuracy:\", acc_4)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"\\nBest 3 Features\")\n",
        "print(\"Accuracy:\", acc_3)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_3, y_pred_3))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_3, y_pred_3, target_names=iris.target_names))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n Best 2 Features \")\n",
        "print(\"Accuracy:\", acc_4)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_2, y_pred_2))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_2, y_pred_2, target_names=iris.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vejKeF-S8AUX",
        "outputId": "05b22d4a-57bf-41fe-9f32-ba5ae9db4227"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n",
            "\n",
            "\n",
            " MinMax (Manual Scaling) \n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0           0.222222          0.625000           0.067797          0.041667\n",
            "1           0.166667          0.416667           0.067797          0.041667\n",
            "2           0.111111          0.500000           0.050847          0.041667\n",
            "3           0.083333          0.458333           0.084746          0.041667\n",
            "4           0.194444          0.666667           0.067797          0.041667\n",
            "\n",
            "\n",
            " Variance \n",
            "sepal length (cm) : 0.05255572702331957\n",
            "sepal width (cm) : 0.0327626543209876\n",
            "petal length (cm) : 0.08892567269941572\n",
            "petal width (cm) : 0.10019668209876544\n",
            "\n",
            "\n",
            "--- Correlation Matrix ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô) ---\n",
            "                   sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "sepal length (cm)           1.000000         -0.117570           0.871754   \n",
            "sepal width (cm)           -0.117570          1.000000          -0.428440   \n",
            "petal length (cm)           0.871754         -0.428440           1.000000   \n",
            "petal width (cm)            0.817941         -0.366126           0.962865   \n",
            "\n",
            "                   petal width (cm)  \n",
            "sepal length (cm)          0.817941  \n",
            "sepal width (cm)          -0.366126  \n",
            "petal length (cm)          0.962865  \n",
            "petal width (cm)           1.000000  \n",
            "\n",
            "\n",
            "--- üî¥ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ (Threshold: > 0.85) üî¥ ---\n",
            "\n",
            "--- ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 1 ---\n",
            "1. Correlation Matrix ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:\n",
            "                   sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "sepal length (cm)           0.000000          0.117570           0.871754   \n",
            "sepal width (cm)            0.117570          0.000000           0.428440   \n",
            "petal length (cm)           0.871754          0.428440           0.000000   \n",
            "petal width (cm)            0.817941          0.366126           0.962865   \n",
            "\n",
            "                   petal width (cm)  \n",
            "sepal length (cm)          0.817941  \n",
            "sepal width (cm)           0.366126  \n",
            "petal length (cm)          0.962865  \n",
            "petal width (cm)           0.000000  \n",
            "\n",
            "2. ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (0.9629): petal length (cm) vs petal width (cm)\n",
            "3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö S.D. (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à):\n",
            "   * **petal length (cm)**: S.D. = **0.29920**\n",
            "   * **petal width (cm)**: S.D. = **0.31760**\n",
            "4. ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏° S.D.):\n",
            "   * **‡πÄ‡∏Å‡πá‡∏ö** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'petal width (cm)' (S.D. **0.31760**)\n",
            "   * **‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå: 'petal length (cm)' (S.D. 0.29920)\n",
            "\n",
            "‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏´‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå ( 0.85 )\n",
            "\n",
            "--- Correlation Matrix ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ---\n",
            "                   sepal length (cm)  sepal width (cm)  petal width (cm)\n",
            "sepal length (cm)           1.000000         -0.117570          0.817941\n",
            "sepal width (cm)           -0.117570          1.000000         -0.366126\n",
            "petal width (cm)            0.817941         -0.366126          1.000000\n",
            "--- üèÅ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ üèÅ ---\n",
            "‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠: ['sepal length (cm)', 'sepal width (cm)', 'petal width (cm)']\n",
            "\n",
            "--- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ ---\n",
            "‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: ['sepal length (cm)', 'sepal width (cm)', 'petal width (cm)']\n",
            "Accuracy (Features Selected by Correlation/SD): 0.8\n",
            "\n",
            "\n",
            " Features \n",
            "Accuracy: 0.9111111111111111\n",
            "\n",
            "\n",
            "Threshold = 0.03\n",
            "Accuracy: 0.9555555555555556\n",
            "\n",
            "\n",
            " Best 3 Features\n",
            "Accuracy: 0.8444444444444444\n",
            "\n",
            "\n",
            " Best 2 Features\n",
            "Accuracy: 1.0\n",
            "\n",
            "\n",
            "\n",
            "Best 3 Features\n",
            "Accuracy: 0.8444444444444444\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  0  0]\n",
            " [ 0 10  5]\n",
            " [ 0  2 13]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        15\n",
            "  versicolor       0.83      0.67      0.74        15\n",
            "   virginica       0.72      0.87      0.79        15\n",
            "\n",
            "    accuracy                           0.84        45\n",
            "   macro avg       0.85      0.84      0.84        45\n",
            "weighted avg       0.85      0.84      0.84        45\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Best 2 Features \n",
            "Accuracy: 1.0\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  0  0]\n",
            " [ 0 15  0]\n",
            " [ 0  0 15]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        15\n",
            "  versicolor       1.00      1.00      1.00        15\n",
            "   virginica       1.00      1.00      1.00        15\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n"
          ]
        }
      ]
    }
  ]
}